import json
import torch
from transformers import GPT2LMHeadModel
import pandas as pd

def main():
    print('ğŸ§ª ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼å®Œæ•´æµ‹è¯•æ¨¡å‹')
    print('=' * 80)
    
    # åŠ è½½è®­ç»ƒæ—¶çš„codebook
    codebook = pd.read_csv('unified_codebook/unified_codebook.csv')
    print(f'âœ… åŠ è½½codebookæˆåŠŸï¼Œå…±{len(codebook)}ä¸ªèšç±»')
    
    # åŠ è½½è®­ç»ƒæ—¶çš„è¯æ±‡è¡¨
    with open('generated_sequences_super_enhanced/vocab.json', 'r') as f:
        vocab = json.load(f)
    inv_vocab = {v: k for k, v in vocab.items()}
    print(f'âœ… åŠ è½½è¯æ±‡è¡¨æˆåŠŸï¼Œå…±{len(vocab)}ä¸ªtoken')
    
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained('outputs/best_model')
    model.eval()
    device = 'cpu'
    model = model.to(device)
    print(f'âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}')
    print(f'ğŸ“Š æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {model.config.vocab_size}')
    print(f'ğŸ“Š è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(vocab)}')
    
    # åŠ è½½æµ‹è¯•æ ·æœ¬
    test_samples = []
    with open('generated_sequences_super_enhanced/sequences_labels_fixed.jsonl', 'r') as f:
        for i, line in enumerate(f):
            if i >= 3:  # å–å‰3ä¸ªæ ·æœ¬
                break
            if line.strip():
                test_samples.append(json.loads(line.strip()))
    print(f'âœ… åŠ è½½æµ‹è¯•æ ·æœ¬æˆåŠŸï¼Œå…±{len(test_samples)}ä¸ªæ ·æœ¬')
    
    # æµ‹è¯•æ¯ä¸ªæ ·æœ¬
    for i, sample in enumerate(test_samples):
        print(f'\nğŸ“Š æµ‹è¯•æ ·æœ¬ {i+1}:')
        print(f'   æ ‡ç­¾: {sample["label"]}')
        print(f'   æ–‡ä»¶å: {sample["meta"]["filename"]}')
        print(f'   åŸå§‹æ•°å­—: {sample["meta"]["original_digit"]}')
        
        # è·å–tokenåºåˆ—
        tokens = sample['tokens'].split()
        print(f'   Tokenåºåˆ—é•¿åº¦: {len(tokens)}')
        
        # æ‰¾åˆ°<CLS>ä½ç½®ï¼ˆè®­ç»ƒæ—¶çš„æ ¼å¼ï¼‰
        try:
            cls_pos = tokens.index('<CLS>')
            input_tokens = tokens[:cls_pos+1]
        except ValueError:
            print('   âš ï¸ æœªæ‰¾åˆ°<CLS>tokenï¼Œä½¿ç”¨å‰400ä¸ªtoken')
            input_tokens = tokens[:400]
        
        # è½¬æ¢ä¸ºIDåºåˆ—
        input_ids = [vocab.get(t, vocab['<UNK>']) for t in input_tokens]
        
        # æˆªæ–­åˆ°æ¨¡å‹æœ€å¤§ä½ç½®
        max_len = min(len(input_ids), 400)
        input_ids = input_ids[:max_len]
        
        print(f'   è¾“å…¥é•¿åº¦: {len(input_ids)}')
        print(f'   è¾“å…¥tokenså‰5ä¸ª: {" ".join(input_tokens[:5])}...')
        
        # è¿›è¡Œæ¨ç†
        input_tensor = torch.tensor([input_ids], dtype=torch.long)
        with torch.no_grad():
            outputs = model(input_tensor)
            logits = outputs.logits[0, -1, :]
            probs = torch.softmax(logits, dim=-1)
            top_probs, top_indices = torch.topk(probs, 5)
        
        print(f'   ğŸ”® ä¸‹ä¸€ä¸ªtokençš„top-5é¢„æµ‹:')
        for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):
            idx_val = idx.item()
            token_name = inv_vocab.get(idx_val, f'UNK_{idx_val}')
            print(f'     {j+1}. {token_name}: æ¦‚ç‡ {prob.item():.4f}')
        
        # æ£€æŸ¥æœŸæœ›æ ‡ç­¾
        expected_label = sample['label']
        if f'<CLS_{expected_label}>' in vocab:
            expected_id = vocab[f'<CLS_{expected_label}>']
            expected_prob = probs[expected_id].item()
            print(f'   ğŸ¯ æœŸæœ›æ ‡ç­¾: <CLS_{expected_label}> (ID: {expected_id}), æ¦‚ç‡: {expected_prob:.4f}')
        elif str(expected_label) in vocab:
            expected_id = vocab[str(expected_label)]
            expected_prob = probs[expected_id].item()
            print(f'   ğŸ¯ æœŸæœ›æ ‡ç­¾: {expected_label} (ID: {expected_id}), æ¦‚ç‡: {expected_prob:.4f}')
        else:
            print(f'   âš ï¸ æœŸæœ›æ ‡ç­¾ä¸åœ¨è¯æ±‡è¡¨ä¸­: {expected_label}')
        
        print('-' * 50)
    
    print('\nğŸš€ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›...')
    # æµ‹è¯•ç”Ÿæˆ
    test_inputs = [
        ['<IMG>', '<Z_100>', '<Z_200>'],
        ['<IMG>', '<Z_369>', '<Z_255>', '<CLS>']
    ]
    
    for i, test_input in enumerate(test_inputs):
        print(f'\nğŸ“ ç”Ÿæˆæµ‹è¯• {i+1}:')
        test_ids = [vocab.get(t, vocab['<UNK>']) for t in test_input]
        test_tensor = torch.tensor([test_ids], dtype=torch.long)
        
        try:
            with torch.no_grad():
                generated = model.generate(
                    test_tensor,
                    max_new_tokens=3,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=vocab['<PAD>'],
                    eos_token_id=vocab.get('<EOS>', vocab['<UNK>'])
                )
            
            generated_tokens = [inv_vocab.get(tid.item(), f'UNK_{tid.item()}') 
                              for tid in generated[0]]
            
            print(f'   è¾“å…¥: {test_input}')
            print(f'   ç”Ÿæˆ: {generated_tokens}')
            print(f'   æ–°å¢: {generated_tokens[len(test_input):]}')
        
        except Exception as e:
            print(f'   âŒ ç”Ÿæˆå¤±è´¥: {e}')
    
    print('\n' + '=' * 80)
    print('âœ… æµ‹è¯•å®Œæˆï¼')
    print('ğŸ“‹ æ€»ç»“:')
    print('  - æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½å’Œæ¨ç†')
    print('  - ä½¿ç”¨äº†è®­ç»ƒæ—¶çš„codebookå’Œè¯æ±‡è¡¨')
    print('  - è¾“å…¥æ ¼å¼ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ˆåˆ°<CLS>æˆªæ­¢ï¼‰')
    print('  - å¯ä»¥è¿›è¡Œnext-tokené¢„æµ‹å’Œæ–‡æœ¬ç”Ÿæˆ')

if __name__ == "__main__":
    main()
