#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨è®­ç»ƒæ—¶çš„codebookå’Œè¯æ±‡è¡¨æµ‹è¯•æ¨¡å‹
"""

import json
import torch
from transformers import GPT2LMHeadModel
import pandas as pd

def main():
    print('ğŸ§ª ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹ï¼ˆCPUæ¨¡å¼ï¼‰...')
    print('=' * 80)
    
    # åŠ è½½è®­ç»ƒæ—¶çš„codebook
    print('ğŸ“¥ åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„codebook...')
    codebook = pd.read_csv('unified_codebook/unified_codebook.csv')
    print(f'âœ… åŠ è½½codebookæˆåŠŸï¼Œå…±{len(codebook)}ä¸ªèšç±»')
    
    # åŠ è½½è®­ç»ƒæ—¶çš„è¯æ±‡è¡¨
    print('ğŸ“¥ åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯æ±‡è¡¨...')
    with open('generated_sequences_super_enhanced/vocab.json', 'r') as f:
        vocab = json.load(f)
    print(f'âœ… åŠ è½½è¯æ±‡è¡¨æˆåŠŸï¼Œå…±{len(vocab)}ä¸ªtoken')
    
    # åˆ›å»ºåå‘è¯æ±‡è¡¨ï¼ˆID -> Tokenï¼‰
    inv_vocab = {v: k for k, v in vocab.items()}
    
    # åŠ è½½æµ‹è¯•æ ·æœ¬
    print('ğŸ“¥ åŠ è½½æµ‹è¯•æ ·æœ¬...')
    test_samples = []
    with open('generated_sequences_super_enhanced/sequences_labels_fixed.jsonl', 'r') as f:
        for i, line in enumerate(f):
            if i >= 3:  # åªå–å‰3ä¸ªæ ·æœ¬
                break
            if line.strip():
                data = json.loads(line.strip())
                test_samples.append(data)
    print(f'âœ… åŠ è½½æµ‹è¯•æ ·æœ¬æˆåŠŸï¼Œå…±{len(test_samples)}ä¸ªæ ·æœ¬')
    
    # åŠ è½½æ¨¡å‹
    print('ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...')
    model = GPT2LMHeadModel.from_pretrained('outputs/best_model')
    device = 'cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
    model = model.to(device)
    model.eval()
    print(f'âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}')
    
    # è·å–æ¨¡å‹çš„æœ€å¤§ä½ç½®ç¼–ç 
    max_positions = getattr(model.config, 'n_positions', 512)
    print(f'ğŸ“Š æ¨¡å‹æœ€å¤§ä½ç½®ç¼–ç : {max_positions}')
    print(f'ğŸ“Š æ¨¡å‹è¯æ±‡è¡¨å¤§å°: {model.config.vocab_size}')
    print(f'ğŸ“Š è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(vocab)}')
    
    # æµ‹è¯•æ¯ä¸ªæ ·æœ¬
    for i, sample in enumerate(test_samples):
        print(f'\nğŸ“Š æµ‹è¯•æ ·æœ¬ {i+1}:')
        print(f'   æ ‡ç­¾: {sample["label"]}')
        print(f'   æ–‡ä»¶å: {sample["meta"]["filename"]}')
        print(f'   åŸå§‹æ•°å­—: {sample["meta"]["original_digit"]}')
        
        # è·å–tokenåºåˆ—
        tokens_str = sample['tokens']
        tokens = tokens_str.split()
        print(f'   Tokenåºåˆ—é•¿åº¦: {len(tokens)}')
        
        # æ‰¾åˆ°<CLS>ä½ç½®
        cls_pos = -1
        try:
            cls_pos = tokens.index('<CLS>')
        except ValueError:
            print('   âš ï¸ æœªæ‰¾åˆ°<CLS>tokenï¼Œä½¿ç”¨å®Œæ•´åºåˆ—')
            cls_pos = len(tokens)
        
        # è®­ç»ƒæ—¶è¾“å…¥åˆ°<CLS>æˆªæ­¢
        input_tokens = tokens[:cls_pos+1] if cls_pos < len(tokens) else tokens
        
        # è½¬æ¢ä¸ºIDåºåˆ—
        input_ids = []
        for token in input_tokens:
            if token in vocab:
                input_ids.append(vocab[token])
            else:
                input_ids.append(vocab['<UNK>'])
        
        print(f'   è®­ç»ƒè¾“å…¥é•¿åº¦: {len(input_ids)}')
        
        # æˆªæ–­åˆ°æ¨¡å‹æœ€å¤§ä½ç½®ç¼–ç ï¼ˆç•™ä¸€äº›ä½™é‡ï¼‰
        max_len = min(len(input_ids), max_positions - 1)
        input_ids = input_ids[:max_len]
        
        print(f'   æˆªæ–­åé•¿åº¦: {len(input_ids)}')
        print(f'   è¾“å…¥tokenså‰10ä¸ª: {" ".join(input_tokens[:10])}...')
        
        # è½¬æ¢ä¸ºtensor
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        # è¿›è¡Œæ¨ç†
        try:
            with torch.no_grad():
                outputs = model(input_tensor)
                logits = outputs.logits
                
                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
                next_token_logits = logits[0, -1, :]
                probs = torch.softmax(next_token_logits, dim=-1)
                
                # è·å–top-5é¢„æµ‹
                top_k = 5
                top_probs, top_indices = torch.topk(probs, top_k)
                
                print(f'   ğŸ”® ä¸‹ä¸€ä¸ªtokençš„top-{top_k}é¢„æµ‹:')
                for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):
                    idx_val = idx.item()
                    token_name = inv_vocab.get(idx_val, f'<UNK_ID_{idx_val}>')
                    print(f'     {j+1}. {token_name} (ID: {idx_val}): æ¦‚ç‡ {prob.item():.4f}')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœŸæœ›çš„æ ‡ç­¾token
                expected_label = sample['label']
                # æŸ¥æ‰¾å¯èƒ½çš„æ ‡ç­¾tokenæ ¼å¼
                possible_labels = [f'<CLS_{expected_label}>', f'{expected_label}', str(expected_label)]
                
                found_expected = False
                for expected_token in possible_labels:
                    if expected_token in vocab:
                        expected_id = vocab[expected_token]
                        expected_prob = probs[expected_id].item()
                        print(f'   ğŸ¯ æœŸæœ›æ ‡ç­¾: {expected_token} (ID: {expected_id}), æ¦‚ç‡: {expected_prob:.4f}')
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨top-5ä¸­
                        if expected_id in top_indices:
                            rank = (top_indices == expected_id).nonzero(as_tuple=True)[0].item()
                            print(f'   âœ… æœŸæœ›æ ‡ç­¾åœ¨top-{top_k}ä¸­ï¼Œæ’å: {rank+1}')
                        else:
                            print(f'   âŒ æœŸæœ›æ ‡ç­¾ä¸åœ¨top-{top_k}ä¸­')
                        found_expected = True
                        break
                
                if not found_expected:
                    print(f'   âš ï¸ æœªæ‰¾åˆ°æœŸæœ›æ ‡ç­¾tokenï¼ˆå°è¯•äº†: {possible_labels}ï¼‰')
            
        except Exception as e:
            print(f'   âŒ æ¨ç†å¤±è´¥: {e}')
        
        print('-' * 80)
    
    print('\nâœ… æµ‹è¯•å®Œæˆï¼')
    
    # ç®€å•çš„ç”Ÿæˆæµ‹è¯•
    print('\nğŸš€ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›...')
    try:
        # ä½¿ç”¨ç®€å•çš„è¾“å…¥åºåˆ—
        test_input = ['<IMG>', '<Z_100>', '<Z_200>', '<CLS>']
        test_ids = [vocab.get(token, vocab['<UNK>']) for token in test_input]
        test_tensor = torch.tensor([test_ids], dtype=torch.long).to(device)
        
        with torch.no_grad():
            generated = model.generate(
                test_tensor,
                max_new_tokens=3,
                temperature=0.8,
                do_sample=True,
                pad_token_id=vocab['<PAD>'],
                eos_token_id=vocab.get('<EOS>', vocab['<UNK>'])
            )
            
            # è½¬æ¢å›tokenåç§°
            generated_tokens = []
            for token_id in generated[0]:
                token_name = inv_vocab.get(token_id.item(), f'<UNK_ID_{token_id.item()}>')
                generated_tokens.append(token_name)
            
            print(f'   ğŸ“ è¾“å…¥: {test_input}')
            print(f'   ğŸ¯ ç”Ÿæˆç»“æœ: {generated_tokens}')
            print(f'   ğŸ†• æ–°å¢tokens: {generated_tokens[len(test_input):]}')
    
    except Exception as e:
        print(f'   âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}')

if __name__ == "__main__":
    main()
