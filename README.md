# å¤šç»´å›¾ç‰‡åˆ†æLLMé¡¹ç›®

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®ä½¿ç”¨ HuggingFace Transformers ä»é›¶è®­ç»ƒä¸€ä¸ªå°å‹ GPT è¯­è¨€æ¨¡å‹ï¼Œå®ç°**å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal LMï¼‰**ä»»åŠ¡ï¼šç»™å®šå‰æ–‡tokensåºåˆ—ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenã€‚é¡¹ç›®åŒ…å«ä¸¤æ¬¡è®­ç»ƒç‰ˆæœ¬ï¼Œå·²æŒ‰è®­ç»ƒè½®æ¬¡å½’æ¡£ã€‚

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ training_v1/                    # ç¬¬ä¸€æ¬¡è®­ç»ƒç‰ˆæœ¬
â”‚   â”œâ”€â”€ train_gpt.py               # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ inference.py               # æ¨ç†è„šæœ¬ï¼ˆå·²ä¿®å¤ï¼‰
â”‚   â”œâ”€â”€ requirements.txt           # ä¾èµ–åŒ…åˆ—è¡¨
â”‚   â”œâ”€â”€ config.yaml               # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ README.md                 # è¯¦ç»†é¡¹ç›®è¯´æ˜
â”‚   â”œâ”€â”€ outputs/                  # è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ best_model/          # æœ€ä½³éªŒè¯æ€§èƒ½çš„æ¨¡å‹
â”‚   â”‚   â””â”€â”€ final_model/         # æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹
â”‚   â”œâ”€â”€ generated_sequences_super_enhanced/
â”‚   â”‚   â”œâ”€â”€ sequences_labels_fixed.jsonl  # è®­ç»ƒæ•°æ®
â”‚   â”‚   â””â”€â”€ vocab.json           # è¯æ±‡è¡¨
â”‚   â”œâ”€â”€ training_plots/          # è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
â”‚   â””â”€â”€ unified_codebook/        # ç æœ¬ä¸ç›¸å…³ç»Ÿè®¡
â”‚
â””â”€â”€ training_v2/                    # ç¬¬äºŒæ¬¡è®­ç»ƒç‰ˆæœ¬ï¼ˆæ•°æ®ä¿®æ­£ç‰ˆï¼‰
    â””â”€â”€ generated_sequences_super_enhanced/
        â””â”€â”€ sequences_labels_fixed_tail_fixed.jsonl  # ä¿®æ­£åçš„è®­ç»ƒæ•°æ®
```

## ğŸ¯ ä¸»è¦ä»»åŠ¡ï¼šå› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal LMï¼‰

- **ç›®æ ‡**: ç»™å®šå‰æ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼ˆé€ä½ç½®å³ç§»æ ‡ç­¾ï¼‰
- **é€‚ç”¨åœºæ™¯**: æ¨¡å‹å¯ä»¥"ç»­å†™/ç”Ÿæˆ"tokenåºåˆ—
- **è®­ç»ƒæ–¹å¼**: LMå¤´ + äº¤å‰ç†µæŸå¤±ï¼Œå¯¹æ¯ä¸ªä½ç½®è¿›è¡Œç›‘ç£ï¼ˆpadä½ç½®è®¾label=-100ï¼‰
- **è¯„ä¼°æŒ‡æ ‡**: Perplexity (PPL)ã€next-token accuracy
- **æ¨ç†æ–¹å¼**: è‡ªå›å½’è§£ç ï¼ˆgreedy/top-k/nucleus samplingï¼‰

## ğŸ”„ ç¬¬äºŒæ¬¡è®­ç»ƒï¼ˆæ•°æ®ä¿®æ­£ç‚¹ï¼‰

### ä¸»è¦æ”¹è¿›
- **å°¾éƒ¨æ ¼å¼å¯¹é½åˆ†ç±»ç›‘ç£**: ç”±"`â€¦ <CLS> <CLS> <EOS>`"ä¿®æ­£ä¸º"`â€¦ <CLS> <CLS_{label}> <EOS>`"
- **é•¿åº¦ä¸æˆªæ–­ç­–ç•¥**: æœ€å¤§é•¿åº¦ 512ï¼Œæ°¸è¿œä¿ç•™ç»“å°¾"`</IMG> <CLS> <CLS_{label}> <EOS>`"
- **è¯è¡¨ä¸tokenizerä¸€è‡´**: `<CLS>`ã€`<EOS>`ã€`<CLS_0>â€¦<CLS_9>` å‡å­˜åœ¨ä¸”IDç¨³å®š

### ä½¿ç”¨ä¿®æ­£æ•°æ®è¿›è¡Œè®­ç»ƒ

```bash
# è¿›å…¥training_v1ç›®å½•
cd training_v1

# ä½¿ç”¨ä¿®æ­£åçš„æ•°æ®è¿›è¡Œç¬¬äºŒæ¬¡è®­ç»ƒ
python train_gpt.py \
  --data_path ../training_v2/generated_sequences_super_enhanced/sequences_labels_fixed_tail_fixed.jsonl \
  --vocab_path generated_sequences_super_enhanced/vocab.json \
  --batch_size 16 \
  --num_epochs 30 \
  --learning_rate 1e-4 \
  --max_length 512
```

### è®­ç»ƒè¦ç‚¹
- è®­ç»ƒè¾“å…¥åˆ° `<CLS>` æˆªæ­¢ï¼Œæ¨¡å‹å­¦ä¹ é¢„æµ‹ä¸‹ä¸€tokenä¸º `<CLS_{label}>`
- è¯„ä¼°æ—¶å– `<CLS>` åä¸€æ­¥çš„åˆ†å¸ƒï¼Œä»…åœ¨ `<CLS_0>â€¦<CLS_9>` ä¸Šå–æœ€å¤§ä½œä¸ºç±»åˆ«
- è®­ç»ƒ/éªŒè¯éœ€ä¿æŒç›¸åŒçš„ `max_length`ã€æˆªæ–­ä¸paddingç­–ç•¥

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd training_v1
pip install -r requirements.txt
```

### 2. ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†

```bash
cd training_v1
python inference.py --model_path outputs/best_model --mode generate
```

### 3. é‡æ–°è®­ç»ƒæ¨¡å‹

```bash
cd training_v1
python train_gpt.py \
  --data_path ../training_v2/generated_sequences_super_enhanced/sequences_labels_fixed_tail_fixed.jsonl \
  --vocab_path generated_sequences_super_enhanced/vocab.json \
  --batch_size 16 \
  --num_epochs 30 \
  --learning_rate 1e-4 \
  --max_length 512
```

## ğŸ† æ¨¡å‹æ€§èƒ½

### ç¬¬ä¸€æ¬¡è®­ç»ƒç»“æœ
- **è®­ç»ƒæŸå¤±**: 0.0095
- **éªŒè¯æŸå¤±**: 0.0100  
- **è®­ç»ƒå›°æƒ‘åº¦ (PPL)**: 1.01
- **éªŒè¯å›°æƒ‘åº¦ (PPL)**: 1.01
- **æ€»æ”¹å–„ç‡**: 99.38%

### æ¨¡å‹æ¶æ„
- **æ¨¡å‹ç±»å‹**: GPT-2 æ¶æ„ï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰
- **å±‚æ•°**: 6å±‚ Transformer
- **æ³¨æ„åŠ›å¤´æ•°**: 8å¤´
- **åµŒå…¥ç»´åº¦**: 384
- **è¯æ±‡è¡¨å¤§å°**: 516 tokens
- **å‚æ•°æ•°é‡**: ~850ä¸‡

## ğŸ“Š è®­ç»ƒæ•°æ®

- **æ•°æ®æ ¼å¼**: JSONLæ–‡ä»¶
- **è¯æ±‡è¡¨**: JSONæ ¼å¼ï¼Œ516ä¸ªç‹¬ç‰¹token
- **åºåˆ—é•¿åº¦**: æœ€å¤§1024 tokens
- **æ•°æ®åˆ’åˆ†**: 80%è®­ç»ƒï¼Œ20%éªŒè¯

## ğŸ”§ ç¯å¢ƒè¦æ±‚

- Python 3.9+
- PyTorch 2.0+
- Transformers 4.41+
- CUDAæ”¯æŒï¼ˆå¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿï¼‰
- æ”¯æŒ bf16 æ··åˆç²¾åº¦è®­ç»ƒ

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåä¼šç”ŸæˆHuggingFaceæ ‡å‡†æ ¼å¼çš„æ¨¡å‹ï¼š

```
outputs/
â”œâ”€â”€ best_model/           # æœ€ä½³éªŒè¯æ€§èƒ½çš„æ¨¡å‹
â”‚   â”œâ”€â”€ config.json       # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ pytorch_model.bin # æ¨¡å‹æƒé‡
â”‚   â”œâ”€â”€ tokenizer.json    # tokenizeré…ç½®
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ training_args.bin # è®­ç»ƒçŠ¶æ€
â””â”€â”€ final_model/          # æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ special_tokens_map.json
```

## ğŸŒ æ¨¡å‹å‘å¸ƒ

æ¨¡å‹å·²å‘å¸ƒåˆ°Hugging Face Hubï¼š
**https://huggingface.co/ludandaye/gpt-causal-lm**

### ä½¿ç”¨æ–¹æ³•

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½æ¨¡å‹å’Œtokenizer
model = GPT2LMHeadModel.from_pretrained("ludandaye/gpt-causal-lm")
tokenizer = GPT2Tokenizer.from_pretrained("ludandaye/gpt-causal-lm")

# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ï¼ˆå»ºè®®8GB+ï¼‰
2. è®­ç»ƒæ—¶é—´çº¦1-3å°æ—¶ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
3. æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
4. æ”¯æŒCPUè®­ç»ƒï¼ˆä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**: å‡å°batch_size
2. **è®­ç»ƒä¸æ”¶æ•›**: è°ƒæ•´å­¦ä¹ ç‡æˆ–å¢åŠ è®­ç»ƒè½®æ•°
3. **æ•°æ®åŠ è½½é”™è¯¯**: æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼

### æ€§èƒ½ä¼˜åŒ–

1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯
3. ä½¿ç”¨å¤šGPUè®­ç»ƒ

## ğŸ“š æ‰©å±•åŠŸèƒ½

- æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹é…ç½®
- å¯æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡ï¼ˆBLEUã€ROUGEç­‰ï¼‰
- æ”¯æŒæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼
- å¯é›†æˆåˆ°WebæœåŠ¡ä¸­
- **æ–‡æœ¬ç”Ÿæˆ**: æ”¯æŒä¸åŒé‡‡æ ·ç­–ç•¥ï¼ˆgreedyã€top-kã€nucleusï¼‰
- **åºåˆ—ç»­å†™**: ç»™å®šå‰ç¼€ï¼Œè‡ªåŠ¨ç”Ÿæˆåç»­å†…å®¹
- **æ¨¡å‹å¾®è°ƒ**: æ”¯æŒåœ¨å…¶ä»–é¢†åŸŸæ•°æ®ä¸Šç»§ç»­è®­ç»ƒ

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ‘¨â€ğŸ’» ä½œè€…

[ludandaye](https://huggingface.co/ludandaye)

## ğŸ“– è¯¦ç»†æ–‡æ¡£

- [ç¬¬ä¸€æ¬¡è®­ç»ƒè¯¦ç»†è¯´æ˜](training_v1/README.md)
- [è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–](training_v1/training_plots/)
- [é¡¹ç›®æ€»ç»“](training_v1/PROJECT_SUMMARY.md)


