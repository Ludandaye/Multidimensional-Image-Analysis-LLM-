#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨æµ‹è¯•é›†å†…å®¹æµ‹è¯•æ¨¡å‹ï¼Œç¡®ä¿æ ¼å¼ä¸è®­ç»ƒæ—¶ä¸€è‡´
ä½¿ç”¨è®­ç»ƒæ—¶çš„codebookå’Œè¯æ±‡è¡¨
"""

import json
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import pandas as pd
import numpy as np

def load_training_codebook():
    """åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„codebook"""
    codebook_path = 'unified_codebook/unified_codebook.csv'
    codebook = pd.read_csv(codebook_path)
    print(f"âœ… åŠ è½½codebookæˆåŠŸï¼Œå…±{len(codebook)}ä¸ªèšç±»")
    return codebook

def load_vocab():
    """åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„è¯æ±‡è¡¨"""
    vocab_path = 'generated_sequences_super_enhanced/vocab.json'
    with open(vocab_path, 'r') as f:
        vocab = json.load(f)
    print(f"âœ… åŠ è½½è¯æ±‡è¡¨æˆåŠŸï¼Œå…±{len(vocab)}ä¸ªtoken")
    return vocab

def load_test_samples():
    """åŠ è½½æµ‹è¯•æ ·æœ¬"""
    test_data_path = 'generated_sequences_super_enhanced/sequences_labels_fixed.jsonl'
    samples = []
    
    with open(test_data_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= 5:  # åªå–å‰5ä¸ªæ ·æœ¬
                break
            data = json.loads(line.strip())
            samples.append(data)
    
    print(f"âœ… åŠ è½½æµ‹è¯•æ ·æœ¬æˆåŠŸï¼Œå…±{len(samples)}ä¸ªæ ·æœ¬")
    return samples

def tokenize_sequence(tokens_str, vocab):
    """å°†tokenå­—ç¬¦ä¸²è½¬æ¢ä¸ºIDåºåˆ—"""
    tokens = tokens_str.split()
    token_ids = []
    
    for token in tokens:
        if token in vocab:
            token_ids.append(vocab[token])
        else:
            token_ids.append(vocab['<UNK>'])
    
    return token_ids

def test_model_with_training_data():
    """ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹"""
    print("ğŸ§ª ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹...")
    
    # åŠ è½½å¿…è¦çš„æ•°æ®
    codebook = load_training_codebook()
    vocab = load_vocab()
    test_samples = load_test_samples()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    try:
        model = GPT2LMHeadModel.from_pretrained('outputs/best_model')
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = model.to(device)
        model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æ¯ä¸ªæ ·æœ¬
    for i, sample in enumerate(test_samples):
        print(f"\nğŸ“Š æµ‹è¯•æ ·æœ¬ {i+1}:")
        print(f"   æ ‡ç­¾: {sample['label']}")
        print(f"   æ–‡ä»¶å: {sample['meta']['filename']}")
        print(f"   åŸå§‹æ•°å­—: {sample['meta']['original_digit']}")
        
        # è·å–tokenåºåˆ—
        tokens_str = sample['tokens']
        print(f"   Tokenåºåˆ—é•¿åº¦: {len(tokens_str.split())}")
        
        # è½¬æ¢ä¸ºIDåºåˆ—
        token_ids = tokenize_sequence(tokens_str, vocab)
        print(f"   Token IDåºåˆ—é•¿åº¦: {len(token_ids)}")
        
        # æˆªå–åˆ°<CLS>ä½ç½®ï¼ˆè®­ç»ƒæ—¶çš„æ ¼å¼ï¼‰
        cls_pos = -1
        for j, token in enumerate(tokens_str.split()):
            if token == '<CLS>':
                cls_pos = j
                break
        
        if cls_pos != -1:
            # è®­ç»ƒæ—¶è¾“å…¥åˆ°<CLS>æˆªæ­¢
            input_tokens = tokens_str.split()[:cls_pos+1]
            input_ids = tokenize_sequence(' '.join(input_tokens), vocab)
            
            print(f"   è®­ç»ƒè¾“å…¥é•¿åº¦: {len(input_ids)}")
            print(f"   è®­ç»ƒè¾“å…¥tokens: {' '.join(input_tokens)}")
            
            # è½¬æ¢ä¸ºtensor
            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
            
            # è¿›è¡Œæ¨ç†
            try:
                with torch.no_grad():
                    outputs = model(input_tensor)
                    logits = outputs.logits
                    
                    # è·å–<CLS>åä¸€æ­¥çš„é¢„æµ‹ï¼ˆè®­ç»ƒæ—¶çš„ç›®æ ‡ï¼‰
                    next_token_logits = logits[0, -1, :]
                    probs = torch.softmax(next_token_logits, dim=-1)
                    
                    # è·å–top-5é¢„æµ‹
                    top_k = 5
                    top_probs, top_indices = torch.topk(probs, top_k)
                    
                    print(f"   ğŸ”® <CLS>åä¸€æ­¥çš„top-{top_k}é¢„æµ‹:")
                    for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):
                        # æ‰¾åˆ°å¯¹åº”çš„token
                        token_name = None
                        for name, id_val in vocab.items():
                            if id_val == idx.item():
                                token_name = name
                                break
                        
                        print(f"     {j+1}. {token_name} (ID: {idx.item()}): æ¦‚ç‡ {prob.item():.4f}")
                    
                    # æ£€æŸ¥æ˜¯å¦é¢„æµ‹äº†æ­£ç¡®çš„æ ‡ç­¾token
                    expected_label = sample['label']
                    expected_token = f"<CLS_{expected_label}>"
                    
                    if expected_token in vocab:
                        expected_id = vocab[expected_token]
                        expected_prob = probs[expected_id].item()
                        print(f"   ğŸ¯ æœŸæœ›æ ‡ç­¾: {expected_token} (ID: {expected_id}), æ¦‚ç‡: {expected_prob:.4f}")
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨top-5ä¸­
                        if expected_id in top_indices:
                            rank = (top_indices == expected_id).nonzero(as_tuple=True)[0].item()
                            print(f"   âœ… æœŸæœ›æ ‡ç­¾åœ¨top-{top_k}ä¸­ï¼Œæ’å: {rank+1}")
                        else:
                            print(f"   âŒ æœŸæœ›æ ‡ç­¾ä¸åœ¨top-{top_k}ä¸­")
                    else:
                        print(f"   âš ï¸ æœŸæœ›æ ‡ç­¾token {expected_token} ä¸åœ¨è¯æ±‡è¡¨ä¸­")
                
            except Exception as e:
                print(f"   âŒ æ¨ç†å¤±è´¥: {e}")
        
        print("-" * 80)

def test_text_generation():
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›"""
    print("\nğŸš€ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›...")
    
    try:
        model = GPT2LMHeadModel.from_pretrained('outputs/best_model')
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = model.to(device)
        model.eval()
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹æ®Štoken
        vocab = load_vocab()
        
        # æµ‹è¯•ä¸åŒçš„è¾“å…¥
        test_inputs = [
            ["<IMG>", "<Z_100>", "<Z_200>"],
            ["<IMG>", "<Z_300>", "<Z_400>", "<Z_500>"],
            ["<IMG>", "<Z_100>", "<Z_200>", "<Z_300>", "<CLS>"]
        ]
        
        for i, input_tokens in enumerate(test_inputs):
            print(f"\nğŸ“ æµ‹è¯•è¾“å…¥ {i+1}: {input_tokens}")
            
            # è½¬æ¢ä¸ºID
            input_ids = []
            for token in input_tokens:
                if token in vocab:
                    input_ids.append(vocab[token])
                else:
                    input_ids.append(vocab['<UNK>'])
            
            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
            
            try:
                with torch.no_grad():
                    # ç”Ÿæˆæ–‡æœ¬
                    generated = model.generate(
                        input_tensor,
                        max_new_tokens=5,
                        temperature=0.8,
                        do_sample=True,
                        pad_token_id=vocab['<PAD>'],
                        eos_token_id=vocab['<EOS>']
                    )
                    
                    # è½¬æ¢å›tokenåç§°
                    generated_tokens = []
                    for token_id in generated[0]:
                        for name, id_val in vocab.items():
                            if id_val == token_id.item():
                                generated_tokens.append(name)
                                break
                    
                    print(f"   ğŸ¯ ç”Ÿæˆç»“æœ: {generated_tokens}")
                    print(f"   ğŸ†• æ–°å¢tokens: {generated_tokens[len(input_tokens):]}")
                    
            except Exception as e:
                print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
    
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ§ª ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼æµ‹è¯•æ¨¡å‹")
    print("=" * 80)
    
    # æµ‹è¯•1: ä½¿ç”¨è®­ç»ƒæ•°æ®æ ¼å¼
    test_model_with_training_data()
    
    # æµ‹è¯•2: æ–‡æœ¬ç”Ÿæˆ
    test_text_generation()
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
