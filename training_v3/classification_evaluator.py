#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸæ­£çš„åˆ†ç±»è¯„ä¼°å™¨
åœ¨<CLS>ä½ç½®ç›´æ¥è¿›è¡Œåˆ†ç±»é¢„æµ‹ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå¼è¯„ä¼°
"""

import torch
import numpy as np
import os
from typing import Dict, List, Tuple
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from transformers import GPT2LMHeadModel
from config.model_config import UnifiedConfig
from data_processor_fixed import FixedCausalLMDataset
import logging

logger = logging.getLogger(__name__)

class ClassificationEvaluator:
    """åˆ†ç±»è¯„ä¼°å™¨"""
    
    def __init__(self, model: GPT2LMHeadModel, config: UnifiedConfig, device: str = 'cpu'):
        self.model = model
        self.config = config
        self.device = device
        
        # è·å–åˆ†ç±»ç›¸å…³çš„token ID
        self.cls_token_id = config.vocab[config.special_tokens.cls_token]
        self.cls_label_ids = [config.vocab[cls_token] for cls_token in config.special_tokens.cls_tokens]
        
        logger.info(f"âœ… åˆ†ç±»è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   <CLS> token ID: {self.cls_token_id}")
        logger.info(f"   åˆ†ç±»æ ‡ç­¾ IDs: {self.cls_label_ids}")
    
    def predict_single(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[int, float, np.ndarray]:
        """
        å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹
        
        Args:
            input_ids: è¾“å…¥token IDs
            attention_mask: æ³¨æ„åŠ›æ©ç 
            
        Returns:
            predicted_class: é¢„æµ‹çš„ç±»åˆ« (0-9)
            confidence: é¢„æµ‹ç½®ä¿¡åº¦
            class_probs: æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
        """
        self.model.eval()
        
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits  # [batch_size, seq_len, vocab_size]
            
            # æ‰¾åˆ°<CLS>ä½ç½®
            batch_size = input_ids.size(0)
            cls_positions = []
            
            for b in range(batch_size):
                # æ‰¾åˆ°æœ€åä¸€ä¸ª<CLS>çš„ä½ç½®
                cls_pos = -1
                for pos in range(input_ids.size(1) - 1, -1, -1):
                    if input_ids[b, pos] == self.cls_token_id:
                        cls_pos = pos
                        break
                cls_positions.append(cls_pos)
            
            # åœ¨<CLS>ä½ç½®æå–åˆ†ç±»logits
            predictions = []
            confidences = []
            all_class_probs = []
            
            for b in range(batch_size):
                cls_pos = cls_positions[b]
                if cls_pos == -1:
                    # æ²¡æ‰¾åˆ°<CLS>ï¼Œéšæœºé¢„æµ‹
                    predictions.append(0)
                    confidences.append(0.1)
                    all_class_probs.append(np.ones(10) / 10)
                    continue
                
                # è·å–<CLS>ä½ç½®çš„logits
                cls_logits = logits[b, cls_pos, :]  # [vocab_size]
                
                # åªè€ƒè™‘åˆ†ç±»æ ‡ç­¾çš„logits
                class_logits = cls_logits[self.cls_label_ids]  # [10]
                class_probs = torch.softmax(class_logits, dim=-1).cpu().numpy()
                
                # é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦
                predicted_class = np.argmax(class_probs)
                confidence = class_probs[predicted_class]
                
                predictions.append(predicted_class)
                confidences.append(confidence)
                all_class_probs.append(class_probs)
            
            if batch_size == 1:
                return predictions[0], confidences[0], all_class_probs[0]
            else:
                return predictions, confidences, all_class_probs
    
    def evaluate_dataset(self, dataset: FixedCausalLMDataset, batch_size: int = 16) -> Dict:
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        from torch.utils.data import DataLoader
        
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        
        all_predictions = []
        all_true_labels = []
        all_confidences = []
        all_class_probs = []
        
        logger.info(f"ğŸ§ª å¼€å§‹è¯„ä¼°æ•°æ®é›†ï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                true_labels = batch['true_label'].tolist()
                
                # æ‰¹é‡é¢„æµ‹
                predictions, confidences, class_probs = self.predict_single(input_ids, attention_mask)
                
                # æ”¶é›†ç»“æœ
                if isinstance(predictions, list):
                    all_predictions.extend(predictions)
                    all_confidences.extend(confidences)
                    all_class_probs.extend(class_probs)
                else:
                    all_predictions.append(predictions)
                    all_confidences.append(confidences)
                    all_class_probs.append(class_probs)
                
                all_true_labels.extend(true_labels)
                
                if (batch_idx + 1) % 10 == 0:
                    logger.info(f"   è¯„ä¼°è¿›åº¦: {(batch_idx + 1) * batch_size}/{len(dataset)}")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        results = self._compute_metrics(all_true_labels, all_predictions, all_confidences, all_class_probs)
        
        return results
    
    def _compute_metrics(self, true_labels: List[int], predictions: List[int], 
                        confidences: List[float], class_probs: List[np.ndarray]) -> Dict:
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        
        # åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, predictions)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(true_labels, predictions, labels=list(range(10)))
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(true_labels, predictions, 
                                           labels=list(range(10)), 
                                           target_names=[f'æ•°å­—{i}' for i in range(10)],
                                           output_dict=True)
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        avg_confidence = np.mean(confidences)
        confidence_by_class = {}
        correct_confidence = []
        incorrect_confidence = []
        
        for true_label, pred_label, conf in zip(true_labels, predictions, confidences):
            if true_label not in confidence_by_class:
                confidence_by_class[true_label] = []
            confidence_by_class[true_label].append(conf)
            
            if true_label == pred_label:
                correct_confidence.append(conf)
            else:
                incorrect_confidence.append(conf)
        
        results = {
            'accuracy': accuracy,
            'confusion_matrix': cm,
            'classification_report': class_report,
            'avg_confidence': avg_confidence,
            'correct_confidence_avg': np.mean(correct_confidence) if correct_confidence else 0,
            'incorrect_confidence_avg': np.mean(incorrect_confidence) if incorrect_confidence else 0,
            'confidence_by_class': {k: np.mean(v) for k, v in confidence_by_class.items()},
            'total_samples': len(true_labels),
            'correct_predictions': sum(1 for t, p in zip(true_labels, predictions) if t == p)
        }
        
        return results
    
    def print_evaluation_results(self, results: Dict):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("=" * 80)
        print("ğŸ“Š åˆ†ç±»è¯„ä¼°ç»“æœ")
        print("=" * 80)
        
        print(f"ğŸ¯ æ€»ä½“å‡†ç¡®ç‡: {results['accuracy']:.2%} ({results['correct_predictions']}/{results['total_samples']})")
        print(f"ğŸ”® å¹³å‡ç½®ä¿¡åº¦: {results['avg_confidence']:.4f}")
        print(f"âœ… æ­£ç¡®é¢„æµ‹ç½®ä¿¡åº¦: {results['correct_confidence_avg']:.4f}")
        print(f"âŒ é”™è¯¯é¢„æµ‹ç½®ä¿¡åº¦: {results['incorrect_confidence_avg']:.4f}")
        
        print(f"\nğŸ“‹ å„ç±»åˆ«å‡†ç¡®ç‡:")
        for i in range(10):
            class_metrics = results['classification_report'][f'æ•°å­—{i}']
            precision = class_metrics['precision']
            recall = class_metrics['recall']
            f1 = class_metrics['f1-score']
            support = class_metrics['support']
            confidence = results['confidence_by_class'].get(i, 0)
            
            print(f"  æ•°å­—{i}: P={precision:.2f} R={recall:.2f} F1={f1:.2f} "
                  f"æ ·æœ¬={support:2d} ç½®ä¿¡åº¦={confidence:.3f}")
        
        print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µ:")
        print("     ", end="")
        for i in range(10):
            print(f"{i:4d}", end="")
        print()
        
        cm = results['confusion_matrix']
        for i in range(10):
            print(f"{i:2d}: ", end="")
            for j in range(10):
                print(f"{cm[i,j]:4d}", end="")
            print()
        
        # åˆ†ææœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«
        print(f"\nğŸ” æœ€å®¹æ˜“æ··æ·†çš„æ•°å­—å¯¹:")
        confusion_pairs = []
        for i in range(10):
            for j in range(10):
                if i != j and cm[i, j] > 0:
                    confusion_pairs.append((i, j, cm[i, j]))
        
        confusion_pairs.sort(key=lambda x: x[2], reverse=True)
        for rank, (true_class, pred_class, count) in enumerate(confusion_pairs[:5], 1):
            print(f"  {rank}. æ•°å­—{true_class} â†’ æ•°å­—{pred_class}: {count}æ¬¡")

def evaluate_model(model_path: str, config: UnifiedConfig, device: str = 'cpu'):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.to(device)
    model.eval()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ClassificationEvaluator(model, config, device)
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    from data_processor_fixed import create_datasets
    _, val_dataset = create_datasets(config)
    
    # è¯„ä¼°
    results = evaluator.evaluate_dataset(val_dataset, batch_size=8)
    
    # æ‰“å°ç»“æœ
    evaluator.print_evaluation_results(results)
    
    return results

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    from config.model_config import get_config
    
    config = get_config()
    
    # è¯„ä¼°å½“å‰æ¨¡å‹
    if os.path.exists('outputs/best_model'):
        print("ğŸ§ª è¯„ä¼°å½“å‰è®­ç»ƒçš„æ¨¡å‹...")
        results = evaluate_model('outputs/best_model', config)
    else:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
