#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„æ¨¡å‹é…ç½®æ–‡ä»¶
è§£å†³ç‰¹æ®Šç¬¦å·ä¸ä¸€è‡´é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°éƒ½ä½¿ç”¨ç›¸åŒçš„é…ç½®
"""

import json
import os
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class SpecialTokensConfig:
    """ç‰¹æ®Štokené…ç½®"""
    # åŸºç¡€ç‰¹æ®Štokenï¼ˆæ ¹æ®å®é™…è¯æ±‡è¡¨ï¼‰
    pad_token: str = "<PAD>"      # ID: 0
    unk_token: str = "<UNK>"      # ID: 1
    eos_token: str = "<EOS>"      # ID: 5
    
    # å›¾åƒç›¸å…³token
    img_start_token: str = "<IMG>"    # ID: 2
    img_end_token: str = "</IMG>"     # ID: 3
    
    # åˆ†ç±»ç›¸å…³token
    cls_token: str = "<CLS>"          # ID: 4
    cls_tokens: List[str] = None      # <CLS_0>, <CLS_1>, ..., <CLS_9> (ID: 506-515)
    
    def __post_init__(self):
        if self.cls_tokens is None:
            self.cls_tokens = [f"<CLS_{i}>" for i in range(10)]
    
    def get_all_special_tokens(self) -> List[str]:
        """è·å–æ‰€æœ‰ç‰¹æ®Štokenåˆ—è¡¨"""
        tokens = [
            self.pad_token,
            self.unk_token,
            self.eos_token,
            self.img_start_token,
            self.img_end_token,
            self.cls_token
        ]
        tokens.extend(self.cls_tokens)
        return tokens
    
    def get_token_ids(self, vocab: Dict[str, int]) -> Dict[str, int]:
        """è·å–ç‰¹æ®Štokençš„IDæ˜ å°„"""
        token_ids = {}
        for token_name, token_value in asdict(self).items():
            if isinstance(token_value, str) and token_value in vocab:
                token_ids[token_name] = vocab[token_value]
            elif isinstance(token_value, list):
                token_ids[token_name] = [vocab.get(t, vocab.get(self.unk_token, 1)) for t in token_value]
        return token_ids

@dataclass
class ModelArchConfig:
    """æ¨¡å‹æ¶æ„é…ç½®"""
    model_type: str = "gpt2"
    vocab_size: int = 516  # å®é™…è¯æ±‡è¡¨å¤§å°
    n_positions: int = 512
    n_ctx: int = 512
    n_embd: int = 384
    n_layer: int = 6
    n_head: int = 8
    max_length: int = 512
    
    # è®­ç»ƒç›¸å…³
    batch_size: int = 16
    learning_rate: float = 1e-4
    num_epochs: int = 30
    warmup_steps: int = 100

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    train_data_path: str = "generated_sequences_super_enhanced/sequences_labels_fixed_v2.jsonl"
    vocab_path: str = "generated_sequences_super_enhanced/vocab.json"
    codebook_path: str = "unified_codebook/unified_codebook.csv"
    
    # æ•°æ®å¤„ç†å‚æ•°
    max_length: int = 512
    train_ratio: float = 0.8
    val_ratio: float = 0.2
    
    # æˆªæ–­ç­–ç•¥ï¼šä»å·¦è¾¹æˆªæ–­ï¼Œä¿ç•™å…³é”®å°¾éƒ¨
    truncation_strategy: str = "left"  # "left" æˆ– "right"
    preserve_tail_tokens: int = 10  # ä¿ç•™æœ€åNä¸ªtoken

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®å’Œå…ƒä¿¡æ¯"""
    experiment_name: str = "training_v2_fixed"
    experiment_version: str = "2.1"
    created_time: str = None
    
    # æ•°æ®ç‰ˆæœ¬ä¿¡æ¯
    data_version: str = "sequences_labels_fixed_tail_fixed"
    vocab_version: str = "v1_516_tokens"
    
    # æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯
    model_version: str = "gpt2_6layer_384dim"
    
    def __post_init__(self):
        if self.created_time is None:
            self.created_time = datetime.now().isoformat()

class UnifiedConfig:
    """ç»Ÿä¸€é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.special_tokens = SpecialTokensConfig()
        self.model = ModelArchConfig()
        self.data = DataConfig()
        self.experiment = ExperimentConfig()
        
        # åŠ è½½è¯æ±‡è¡¨
        self.vocab = self._load_vocab()
        
        # æ›´æ–°æ¨¡å‹é…ç½®ä¸­çš„è¯æ±‡è¡¨å¤§å°
        self.model.vocab_size = len(self.vocab)
        
        # è·å–ç‰¹æ®Štokençš„IDæ˜ å°„
        self.token_ids = self.special_tokens.get_token_ids(self.vocab)
    
    def _load_vocab(self) -> Dict[str, int]:
        """åŠ è½½è¯æ±‡è¡¨"""
        vocab_path = self.data.vocab_path
        if os.path.exists(vocab_path):
            with open(vocab_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®çš„ä¸€è‡´æ€§"""
        issues = []
        
        # æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦éƒ½åœ¨è¯æ±‡è¡¨ä¸­
        for token in self.special_tokens.get_all_special_tokens():
            if token not in self.vocab:
                issues.append(f"ç‰¹æ®Štoken '{token}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
        
        # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°ä¸€è‡´æ€§
        if self.model.vocab_size != len(self.vocab):
            issues.append(f"æ¨¡å‹è¯æ±‡è¡¨å¤§å°({self.model.vocab_size})ä¸å®é™…è¯æ±‡è¡¨å¤§å°({len(self.vocab)})ä¸ä¸€è‡´")
        
        # æ£€æŸ¥åˆ†ç±»tokenæ˜¯å¦å­˜åœ¨ä¸”è¿ç»­
        cls_token_ids = []
        for cls_token in self.special_tokens.cls_tokens:
            if cls_token in self.vocab:
                cls_token_ids.append(self.vocab[cls_token])
            else:
                issues.append(f"åˆ†ç±»token '{cls_token}' ä¸åœ¨è¯æ±‡è¡¨ä¸­")
        
        # æ£€æŸ¥åˆ†ç±»token IDæ˜¯å¦è¿ç»­
        if len(cls_token_ids) == 10:
            cls_token_ids.sort()
            for i in range(1, len(cls_token_ids)):
                if cls_token_ids[i] != cls_token_ids[i-1] + 1:
                    issues.append(f"åˆ†ç±»token IDä¸è¿ç»­: {cls_token_ids}")
        
        if issues:
            print("âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for issue in issues:
                print(f"  - {issue}")
            return False
        else:
            print("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
    
    def save_config(self, save_path: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = {
            'special_tokens': asdict(self.special_tokens),
            'model': asdict(self.model),
            'data': asdict(self.data),
            'experiment': asdict(self.experiment),
            'token_ids': self.token_ids
        }
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_config(self, config_path: str):
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        
        # æ›´æ–°é…ç½®å¯¹è±¡
        self.special_tokens = SpecialTokensConfig(**config_dict['special_tokens'])
        self.model = ModelArchConfig(**config_dict['model'])
        self.data = DataConfig(**config_dict['data'])
        self.experiment = ExperimentConfig(**config_dict['experiment'])
        self.token_ids = config_dict['token_ids']
        
        print(f"âœ… é…ç½®å·²ä»æ–‡ä»¶åŠ è½½: {config_path}")
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("=" * 80)
        print(f"ğŸ”§ å®éªŒé…ç½®æ‘˜è¦: {self.experiment.experiment_name} v{self.experiment.experiment_version}")
        print("=" * 80)
        
        print(f"ğŸ“Š æ¨¡å‹æ¶æ„:")
        print(f"  - ç±»å‹: {self.model.model_type}")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {self.model.vocab_size}")
        print(f"  - å±‚æ•°: {self.model.n_layer}, ç»´åº¦: {self.model.n_embd}, å¤´æ•°: {self.model.n_head}")
        print(f"  - æœ€å¤§é•¿åº¦: {self.model.max_length}")
        
        print(f"ğŸ“ ç‰¹æ®ŠToken:")
        print(f"  - PAD: {self.special_tokens.pad_token} (ID: {self.token_ids.get('pad_token', 'N/A')})")
        print(f"  - EOS: {self.special_tokens.eos_token} (ID: {self.token_ids.get('eos_token', 'N/A')})")
        print(f"  - CLS: {self.special_tokens.cls_token} (ID: {self.token_ids.get('cls_token', 'N/A')})")
        print(f"  - åˆ†ç±»æ ‡ç­¾: {self.special_tokens.cls_tokens[:3]}...{self.special_tokens.cls_tokens[-1]}")
        
        print(f"ğŸ“ æ•°æ®é…ç½®:")
        print(f"  - è®­ç»ƒæ•°æ®: {self.data.train_data_path}")
        print(f"  - è¯æ±‡è¡¨: {self.data.vocab_path}")
        print(f"  - æˆªæ–­ç­–ç•¥: {self.data.truncation_strategy}")
        print(f"  - ä¿ç•™å°¾éƒ¨: {self.data.preserve_tail_tokens} tokens")
        
        print(f"ğŸ§ª å®éªŒä¿¡æ¯:")
        print(f"  - æ•°æ®ç‰ˆæœ¬: {self.experiment.data_version}")
        print(f"  - æ¨¡å‹ç‰ˆæœ¬: {self.experiment.model_version}")
        print(f"  - åˆ›å»ºæ—¶é—´: {self.experiment.created_time}")

# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
def get_config() -> UnifiedConfig:
    """è·å–ç»Ÿä¸€é…ç½®å®ä¾‹"""
    return UnifiedConfig()

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    config = get_config()
    config.validate_config()
    config.print_summary()
    
    # ä¿å­˜é…ç½®ç¤ºä¾‹
    config.save_config("config/unified_config.json")
