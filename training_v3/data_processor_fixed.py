#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤åçš„æ•°æ®å¤„ç†å™¨
è§£å†³ç›‘ç£ç›®æ ‡å¯¹é½ã€æˆªæ–­ç­–ç•¥ã€æ•°æ®æ³„æ¼ç­‰é—®é¢˜
"""

import json
import torch
import hashlib
import random
from typing import List, Dict, Tuple, Any
from torch.utils.data import Dataset
from config.model_config import UnifiedConfig
import logging

logger = logging.getLogger(__name__)

class FixedCausalLMDataset(Dataset):
    """ä¿®å¤åçš„å› æœè¯­è¨€æ¨¡å‹æ•°æ®é›†"""
    
    def __init__(self, data_path: str, config: UnifiedConfig, split: str = "train"):
        self.config = config
        self.split = split
        self.max_length = config.model.max_length
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        self.data = self.load_and_process_data(data_path)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        self.validate_data()
        
        logger.info(f"âœ… {split}é›†åŠ è½½å®Œæˆ: {len(self.data)}æ¡æ ·æœ¬")
    
    def load_and_process_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½å¹¶å¤„ç†æ•°æ®ï¼Œè§£å†³æ•°æ®æ³„æ¼é—®é¢˜"""
        # 1. åŠ è½½æ‰€æœ‰æ•°æ®
        all_data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line)
                        # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå”¯ä¸€hashç”¨äºåˆ’åˆ†
                        item['sample_hash'] = self._generate_sample_hash(item, line_no)
                        all_data.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ {line_no}: {e}")
        
        logger.info(f"ğŸ“¥ æ€»å…±åŠ è½½ {len(all_data)} æ¡æ•°æ®")
        
        # 2. å»é‡ï¼ˆåŸºäºå†…å®¹hashï¼‰
        unique_data = {}
        for item in all_data:
            content_hash = self._generate_content_hash(item['tokens'])
            if content_hash not in unique_data:
                unique_data[content_hash] = item
            else:
                logger.debug(f"å‘ç°é‡å¤æ ·æœ¬ï¼Œå·²è·³è¿‡")
        
        deduplicated_data = list(unique_data.values())
        logger.info(f"ğŸ”„ å»é‡åå‰©ä½™ {len(deduplicated_data)} æ¡æ•°æ®")
        
        # 3. æŒ‰hashå€¼ç¨³å®šåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        train_data, val_data = self._split_data_by_hash(deduplicated_data)
        
        if self.split == "train":
            return train_data
        else:
            return val_data
    
    def _generate_sample_hash(self, item: Dict, line_no: int) -> str:
        """ä¸ºæ ·æœ¬ç”Ÿæˆå”¯ä¸€hash"""
        # ä½¿ç”¨æ–‡ä»¶åã€æ ‡ç­¾ã€è¡Œå·ç”Ÿæˆç¨³å®šçš„hash
        content = f"{item.get('meta', {}).get('filename', '')}-{item.get('label', '')}-{line_no}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _generate_content_hash(self, tokens: str) -> str:
        """ä¸ºtokenåºåˆ—ç”Ÿæˆå†…å®¹hashç”¨äºå»é‡"""
        return hashlib.md5(tokens.encode()).hexdigest()
    
    def _split_data_by_hash(self, data: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """åŸºäºhashå€¼ç¨³å®šåˆ’åˆ†æ•°æ®é›†"""
        train_data = []
        val_data = []
        
        for item in data:
            # ä½¿ç”¨hashçš„æœ€åä¸€ä½å†³å®šåˆ’åˆ†ï¼ˆç¡®ä¿ç¨³å®šä¸”å¹³è¡¡ï¼‰
            hash_int = int(item['sample_hash'][-1], 16)  # 0-15
            if hash_int < 13:  # çº¦80%ç”¨äºè®­ç»ƒ
                train_data.append(item)
            else:  # çº¦20%ç”¨äºéªŒè¯
                val_data.append(item)
        
        logger.info(f"ğŸ“Š æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†{len(train_data)}æ¡, éªŒè¯é›†{len(val_data)}æ¡")
        return train_data, val_data
    
    def validate_data(self):
        """éªŒè¯æ•°æ®æ ¼å¼çš„æ­£ç¡®æ€§"""
        issues = []
        
        for i, item in enumerate(self.data[:100]):  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬
            tokens = item['tokens'].split()
            
            # æ£€æŸ¥å¿…è¦çš„token
            if self.config.special_tokens.cls_token not in tokens:
                issues.append(f"æ ·æœ¬{i}: ç¼ºå°‘<CLS>æ ‡è®°")
            
            if self.config.special_tokens.eos_token not in tokens:
                issues.append(f"æ ·æœ¬{i}: ç¼ºå°‘<EOS>æ ‡è®°")
            
            # æ£€æŸ¥åˆ†ç±»æ ‡ç­¾æ ¼å¼
            label = item.get('label')
            expected_cls_token = f"<CLS_{label}>"
            if expected_cls_token not in tokens:
                issues.append(f"æ ·æœ¬{i}: ç¼ºå°‘åˆ†ç±»æ ‡ç­¾{expected_cls_token}")
            
            # æ£€æŸ¥åºåˆ—æ ¼å¼: ... <CLS> <CLS_y> <EOS>
            if len(tokens) >= 3:
                if (tokens[-3] == self.config.special_tokens.cls_token and
                    tokens[-2] == expected_cls_token and
                    tokens[-1] == self.config.special_tokens.eos_token):
                    continue  # æ ¼å¼æ­£ç¡®
                else:
                    issues.append(f"æ ·æœ¬{i}: å°¾éƒ¨æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸º <CLS> <CLS_y> <EOS>")
        
        if issues:
            logger.warning(f"âš ï¸ æ•°æ®éªŒè¯å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                logger.warning(f"  - {issue}")
        else:
            logger.info("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        tokens_str = item['tokens']
        label = item['label']
        
        # è½¬æ¢ä¸ºtokenåˆ—è¡¨
        tokens = tokens_str.split()
        
        # åº”ç”¨å·¦æˆªæ–­ç­–ç•¥ï¼Œä¿ç•™å…³é”®å°¾éƒ¨
        processed_tokens = self._apply_left_truncation(tokens)
        
        # è½¬æ¢ä¸ºID
        input_ids = self._tokens_to_ids(processed_tokens)
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆç”¨äºå› æœè¯­è¨€å»ºæ¨¡ï¼‰
        labels = self._create_causal_labels(input_ids, processed_tokens, label)
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        attention_mask = [1] * len(input_ids)
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        input_ids, labels, attention_mask = self._pad_sequences(
            input_ids, labels, attention_mask
        )
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'true_label': label,
            'sample_id': item.get('sample_hash', f'{idx}')
        }
    
    def _apply_left_truncation(self, tokens: List[str]) -> List[str]:
        """åº”ç”¨å·¦æˆªæ–­ç­–ç•¥ï¼Œç¡®ä¿ä¿ç•™å…³é”®å°¾éƒ¨"""
        if len(tokens) <= self.max_length:
            return tokens
        
        # ä¿ç•™æœ€åNä¸ªå…³é”®token
        preserve_count = self.config.data.preserve_tail_tokens
        
        # æ‰¾åˆ°å…³é”®å°¾éƒ¨çš„å¼€å§‹ä½ç½®
        tail_start = len(tokens) - preserve_count
        for i in range(len(tokens) - preserve_count, len(tokens)):
            if i >= 0 and tokens[i] == self.config.special_tokens.cls_token:
                tail_start = i
                break
        
        # è®¡ç®—å¯ç”¨çš„å‰éƒ¨é•¿åº¦
        available_front = self.max_length - (len(tokens) - tail_start)
        
        if available_front <= 0:
            # å¦‚æœå°¾éƒ¨å¤ªé•¿ï¼Œåªä¿ç•™å°¾éƒ¨
            return tokens[tail_start:]
        else:
            # ä¿ç•™å‰éƒ¨åˆ† + å°¾éƒ¨
            return tokens[:available_front] + tokens[tail_start:]
    
    def _tokens_to_ids(self, tokens: List[str]) -> List[int]:
        """å°†tokenè½¬æ¢ä¸ºID"""
        vocab = self.config.vocab
        unk_id = vocab.get(self.config.special_tokens.unk_token, 1)
        
        return [vocab.get(token, unk_id) for token in tokens]
    
    def _create_causal_labels(self, input_ids: List[int], tokens: List[str], true_label: int) -> List[int]:
        """åˆ›å»ºå› æœè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ï¼Œç¡®ä¿<CLS>åé¢„æµ‹<CLS_y>"""
        labels = [-100] * len(input_ids)  # -100è¡¨ç¤ºä¸è®¡ç®—loss
        
        # æ‰¾åˆ°<CLS>çš„ä½ç½®
        cls_id = self.config.vocab.get(self.config.special_tokens.cls_token, -1)
        cls_label_id = self.config.vocab.get(f"<CLS_{true_label}>", -1)
        
        for i in range(len(input_ids) - 1):
            if input_ids[i] == cls_id and i + 1 < len(input_ids):
                # ç¡®ä¿<CLS>åé¢é¢„æµ‹çš„æ˜¯æ­£ç¡®çš„<CLS_y>
                if input_ids[i + 1] == cls_label_id:
                    labels[i] = cls_label_id  # <CLS>ä½ç½®çš„æ ‡ç­¾æ˜¯<CLS_y>
                    logger.debug(f"è®¾ç½®<CLS>ä½ç½®{i}çš„æ ‡ç­¾ä¸º<CLS_{true_label}>")
                break
        
        return labels
    
    def _pad_sequences(self, input_ids: List[int], labels: List[int], attention_mask: List[int]) -> Tuple[List[int], List[int], List[int]]:
        """å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦"""
        pad_id = self.config.vocab.get(self.config.special_tokens.pad_token, 0)
        
        # æˆªæ–­æˆ–å¡«å……
        if len(input_ids) > self.max_length:
            input_ids = input_ids[:self.max_length]
            labels = labels[:self.max_length]
            attention_mask = attention_mask[:self.max_length]
        else:
            pad_length = self.max_length - len(input_ids)
            input_ids.extend([pad_id] * pad_length)
            labels.extend([-100] * pad_length)  # å¡«å……ä½ç½®ä¸è®¡ç®—loss
            attention_mask.extend([0] * pad_length)  # å¡«å……ä½ç½®ä¸å‚ä¸æ³¨æ„åŠ›
        
        return input_ids, labels, attention_mask

def create_datasets(config: UnifiedConfig) -> Tuple[FixedCausalLMDataset, FixedCausalLMDataset]:
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
    train_dataset = FixedCausalLMDataset(
        data_path=config.data.train_data_path,
        config=config,
        split="train"
    )
    
    val_dataset = FixedCausalLMDataset(
        data_path=config.data.train_data_path,
        config=config,
        split="val"
    )
    
    return train_dataset, val_dataset

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®å¤„ç†å™¨
    from config.model_config import get_config
    
    logging.basicConfig(level=logging.INFO)
    
    config = get_config()
    train_dataset, val_dataset = create_datasets(config)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # æµ‹è¯•ä¸€ä¸ªæ ·æœ¬
    sample = train_dataset[0]
    print(f"æ ·æœ¬å½¢çŠ¶:")
    for key, value in sample.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
        else:
            print(f"  {key}: {value}")
    
    # éªŒè¯<CLS>ä½ç½®çš„æ ‡ç­¾è®¾ç½®
    input_ids = sample['input_ids'].tolist()
    labels = sample['labels'].tolist()
    
    cls_id = config.vocab[config.special_tokens.cls_token]
    for i, (inp_id, label) in enumerate(zip(input_ids, labels)):
        if inp_id == cls_id and label != -100:
            cls_token_name = None
            for token, token_id in config.vocab.items():
                if token_id == label:
                    cls_token_name = token
                    break
            print(f"âœ… <CLS>åœ¨ä½ç½®{i}ï¼Œæ ‡ç­¾ä¸º{cls_token_name} (ID: {label})")
