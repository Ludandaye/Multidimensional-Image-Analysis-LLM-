#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»é›¶è®­ç»ƒå°GPTå› æœè¯­è¨€æ¨¡å‹ç”¨äºnext-tokené¢„æµ‹
"""

import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
from tqdm import tqdm
import os
import argparse
from typing import List, Dict, Any
import logging
import time
import datetime
from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn, MofNCompleteColumn, BarColumn, TextColumn, TimeRemainingColumn
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
import pickle

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆ›å»ºå¯Œæ–‡æœ¬æ§åˆ¶å°
console = Console()

def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, best_val_loss, checkpoint_dir):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'best_val_loss': best_val_loss,
        'timestamp': datetime.datetime.now().isoformat()
    }
    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pkl')
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")

def load_checkpoint(checkpoint_dir):
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pkl')
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
        logger.info(f"ä¸Šæ¬¡è®­ç»ƒæ—¶é—´: {checkpoint['timestamp']}")
        logger.info(f"æ¢å¤åˆ°ç¬¬ {checkpoint['epoch']} è½®")
        return checkpoint
    return None

def display_training_stats(epoch, num_epochs, train_loss, val_loss, train_ppl, val_ppl, 
                          best_val_loss, learning_rate, elapsed_time, eta):
    """æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("æŒ‡æ ‡", style="cyan", no_wrap=True)
    table.add_column("æ•°å€¼", style="yellow")
    
    table.add_row("è½®æ¬¡", f"{epoch}/{num_epochs}")
    table.add_row("è®­ç»ƒæŸå¤±", f"{train_loss:.4f}")
    table.add_row("éªŒè¯æŸå¤±", f"{val_loss:.4f}")
    table.add_row("è®­ç»ƒå›°æƒ‘åº¦", f"{train_ppl:.2f}")
    table.add_row("éªŒè¯å›°æƒ‘åº¦", f"{val_ppl:.2f}")
    table.add_row("æœ€ä½³éªŒè¯æŸå¤±", f"{best_val_loss:.4f}")
    table.add_row("å­¦ä¹ ç‡", f"{learning_rate:.2e}")
    table.add_row("å·²ç”¨æ—¶é—´", f"{elapsed_time}")
    table.add_row("é¢„è®¡å‰©ä½™", f"{eta}")
    
    console.print(Panel(table, title=f"[bold green]è®­ç»ƒè¿›åº¦ - ç¬¬ {epoch} è½®[/bold green]", 
                       border_style="green"))

class CausalLMDataset(Dataset):
    """å› æœè¯­è¨€æ¨¡å‹æ•°æ®é›†ç±»"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½JSONLæ•°æ®"""
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        tokens = item['tokens']
        
        # å°†tokensè½¬æ¢ä¸ºtoken IDs
        token_ids = []
        for token in tokens.split():
            if token in self.tokenizer.get_vocab():
                token_ids.append(self.tokenizer.get_vocab()[token])
            else:
                token_ids.append(self.tokenizer.unk_token_id)
        
        # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        else:
            token_ids = token_ids + [self.tokenizer.pad_token_id] * (self.max_length - len(token_ids))
        
        # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelsæ˜¯å‘å³ç§»åŠ¨ä¸€ä½çš„input_ids
        # padä½ç½®çš„labelè®¾ä¸º-100ï¼ˆä¸å‚ä¸æŸå¤±è®¡ç®—ï¼‰
        labels = token_ids[1:] + [-100]  # å³ç§»ä¸€ä½ï¼Œæœ€åä¸€ä¸ªä½ç½®è®¾ä¸º-100
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long),
            'attention_mask': torch.tensor([1 if tid != self.tokenizer.pad_token_id else 0 for tid in token_ids])
        }

class CustomGPT2Config(GPT2Config):
    """è‡ªå®šä¹‰GPT2é…ç½®"""
    
    def __init__(self, vocab_size: int, **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        # å‡å°æ¨¡å‹å¤§å°ï¼Œé€‚åˆä»é›¶è®­ç»ƒ
        self.n_layer = 6  # åŸå§‹æ˜¯12å±‚
        self.n_head = 8   # åŸå§‹æ˜¯12å¤´
        self.n_embd = 384 # åŸå§‹æ˜¯768ç»´
        self.n_positions = 1024
        self.n_ctx = 1024
    
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½é…ç½®"""
        config = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å‚æ•°éƒ½è¢«è®¾ç½®
        if not hasattr(config, 'vocab_size'):
            config.vocab_size = 50257  # GPT2é»˜è®¤è¯æ±‡è¡¨å¤§å°
        return config

class CausalGPT2LM(nn.Module):
    """å› æœè¯­è¨€æ¨¡å‹GPT2"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ä½¿ç”¨æ ‡å‡†çš„GPT2LMHeadModelï¼Œå®ƒå·²ç»åŒ…å«äº†è¯­è¨€å»ºæ¨¡å¤´
        self.transformer = GPT2LMHeadModel(config)
        
        # åˆå§‹åŒ–æƒé‡
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        # ç›´æ¥ä½¿ç”¨GPT2LMHeadModelçš„å‰å‘ä¼ æ’­
        # å®ƒä¼šè‡ªåŠ¨å¤„ç†å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True
        )
        
        return outputs

def create_tokenizer(vocab_path: str, model_name: str = "gpt2"):
    """åˆ›å»ºtokenizer"""
    # åŠ è½½è‡ªå®šä¹‰è¯æ±‡è¡¨
    with open(vocab_path, 'r', encoding='utf-8') as f:
        vocab = json.load(f)
    
    # åˆ›å»ºtokenizer
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    
    # æ·»åŠ ç‰¹æ®Štoken
    special_tokens = ['<PAD>', '<UNK>', '<IMG>', '</IMG>', '<CLS>', '<EOS>']
    tokenizer.add_special_tokens({'pad_token': '<PAD>', 'unk_token': '<UNK>'})
    
    # æ·»åŠ è‡ªå®šä¹‰token
    tokenizer.add_tokens(list(vocab.keys()))
    
    # è®¾ç½®pad_token_id
    tokenizer.pad_token = '<PAD>'
    tokenizer.pad_token_id = vocab['<PAD>']
    
    logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
    return tokenizer

def train_model(model, train_dataloader, val_dataloader, device, num_epochs=10, learning_rate=5e-5, tokenizer=None, config=None, resume_from_checkpoint=True):
    """è®­ç»ƒæ¨¡å‹ - æ”¯æŒæ–­ç‚¹ç»­è®­å’Œè¯¦ç»†è¿›åº¦æ˜¾ç¤º"""
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    best_val_loss = float('inf')
    train_start_time = time.time()
    
    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint_dir = 'outputs/checkpoints'
    if resume_from_checkpoint:
        checkpoint = load_checkpoint(checkpoint_dir)
        if checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_val_loss = checkpoint['best_val_loss']
            console.print(f"[green]âœ… ä»ç¬¬ {checkpoint['epoch']} è½®æ¢å¤è®­ç»ƒ[/green]")
    
    # æ˜¾ç¤ºè®­ç»ƒå¼€å§‹ä¿¡æ¯
    console.print(Panel(f"[bold blue]å¼€å§‹è®­ç»ƒ GPT æ¨¡å‹[/bold blue]\n"
                       f"æ€»è½®æ•°: {num_epochs}\n"
                       f"å¼€å§‹è½®æ•°: {start_epoch + 1}\n"
                       f"å­¦ä¹ ç‡: {learning_rate}\n"
                       f"è®¾å¤‡: {device}", title="è®­ç»ƒé…ç½®", border_style="blue"))
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨Richè¿›åº¦æ¡
        model.train()
        train_loss = 0.0
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            MofNCompleteColumn(),
            TextColumn("â€¢"),
            TimeElapsedColumn(),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
            console=console
        ) as progress:
            train_task = progress.add_task(f"[cyan]ç¬¬ {epoch+1}/{num_epochs} è½® - è®­ç»ƒ", total=len(train_dataloader))
            
            for batch_idx, batch in enumerate(train_dataloader):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
                
                optimizer.zero_grad()
                
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs['loss']
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += loss.item()
                current_ppl = torch.exp(loss).item()
                
                # æ›´æ–°è¿›åº¦æ¡
                progress.update(train_task, advance=1, 
                              description=f"[cyan]ç¬¬ {epoch+1}/{num_epochs} è½® - è®­ç»ƒ Loss: {loss.item():.4f} PPL: {current_ppl:.2f}")
        
        # éªŒè¯é˜¶æ®µ - ä½¿ç”¨Richè¿›åº¦æ¡
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                MofNCompleteColumn(),
                TextColumn("â€¢"),
                TimeElapsedColumn(),
                TextColumn("â€¢"),
                TimeRemainingColumn(),
                console=console
            ) as progress:
                val_task = progress.add_task(f"[yellow]ç¬¬ {epoch+1}/{num_epochs} è½® - éªŒè¯", total=len(val_dataloader))
                
                for batch_idx, batch in enumerate(val_dataloader):
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)
                    
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    loss = outputs['loss']
                    val_loss += loss.item()
                    current_ppl = torch.exp(loss).item()
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress.update(val_task, advance=1,
                                  description=f"[yellow]ç¬¬ {epoch+1}/{num_epochs} è½® - éªŒè¯ Loss: {loss.item():.4f} PPL: {current_ppl:.2f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå›°æƒ‘åº¦
        avg_train_loss = train_loss / len(train_dataloader)
        avg_val_loss = val_loss / len(val_dataloader)
        avg_train_ppl = torch.exp(torch.tensor(avg_train_loss))
        avg_val_ppl = torch.exp(torch.tensor(avg_val_loss))
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        epoch_time = time.time() - epoch_start_time
        total_elapsed = time.time() - train_start_time
        remaining_epochs = num_epochs - epoch - 1
        eta = remaining_epochs * (total_elapsed / (epoch - start_epoch + 1)) if epoch > start_epoch else 0
        
        # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
        elapsed_str = str(datetime.timedelta(seconds=int(total_elapsed)))
        eta_str = str(datetime.timedelta(seconds=int(eta)))
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        display_training_stats(
            epoch + 1, num_epochs, avg_train_loss, avg_val_loss,
            avg_train_ppl, avg_val_ppl, best_val_loss,
            scheduler.get_last_lr()[0], elapsed_str, eta_str
        )
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, avg_val_loss, best_val_loss, checkpoint_dir)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # ä¿å­˜ä¸ºHuggingFaceæ ‡å‡†æ ¼å¼
            best_model_dir = 'outputs/best_model'
            os.makedirs(best_model_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹æƒé‡ - ä½¿ç”¨å†…éƒ¨çš„transformeræ¨¡å‹
            try:
                model.transformer.save_pretrained(best_model_dir)
            except Exception as e:
                logger.warning(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºç°è­¦å‘Š: {e}")
                # ä½¿ç”¨torch.saveä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                torch.save(model.state_dict(), os.path.join(best_model_dir, 'pytorch_model.bin'))
                logger.info("ä½¿ç”¨torch.saveä¿å­˜æ¨¡å‹æƒé‡")
            
            # ä¿å­˜tokenizer
            if tokenizer is not None:
                tokenizer.save_pretrained(best_model_dir)
            
            # ä¿å­˜è®­ç»ƒé…ç½®
            if config is not None:
                try:
                    config.save_pretrained(best_model_dir)
                except Exception as e:
                    logger.warning(f"ä¿å­˜é…ç½®æ—¶å‡ºç°è­¦å‘Š: {e}")
                    # æ‰‹åŠ¨ä¿å­˜é…ç½®
                    config_dict = config.to_dict()
                    with open(os.path.join(best_model_dir, 'config.json'), 'w') as f:
                        json.dump(config_dict, f, indent=2)
                    logger.info("æ‰‹åŠ¨ä¿å­˜é…ç½®æ–‡ä»¶")
            
            console.print(f"[green]ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼éªŒè¯æŸå¤±: {best_val_loss:.4f}, å›°æƒ‘åº¦: {avg_val_ppl:.2f}[/green]")
            logger.info(f'ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {best_model_dir}, éªŒè¯æŸå¤±: {best_val_loss:.4f}, å›°æƒ‘åº¦: {avg_val_ppl:.2f}')
        
        scheduler.step()
        
        # æ·»åŠ åˆ†éš”çº¿
        console.print("â”€" * 80)
    
    return model

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒGPTå› æœè¯­è¨€æ¨¡å‹ç”¨äºnext-tokené¢„æµ‹')
    parser.add_argument('--data_path', type=str, default='generated_sequences_super_enhanced/sequences_labels_fixed.jsonl',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--vocab_path', type=str, default='generated_sequences_super_enhanced/vocab.json',
                       help='è¯æ±‡è¡¨è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_epochs', type=int, default=20, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--max_length', type=int, default=1024, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡é€‰æ‹©')
    
    args = parser.parse_args()
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºtokenizer
    tokenizer = create_tokenizer(args.vocab_path)
    
    # åŠ è½½æ•°æ®
    dataset = CausalLMDataset(args.data_path, tokenizer, args.max_length)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    config = CustomGPT2Config(
        vocab_size=len(tokenizer),
        bos_token_id=tokenizer.bos_token_id,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CausalGPT2LM(config)
    model.to(device)
    
    logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    console.print(Panel("[bold green]å¼€å§‹è®­ç»ƒ GPT æ¨¡å‹[/bold green]", border_style="green"))
    
    trained_model = train_model(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        device=device,
        num_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        tokenizer=tokenizer,
        config=config,
        resume_from_checkpoint=True
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸ºHuggingFaceæ ‡å‡†æ ¼å¼
    final_model_dir = 'outputs/final_model'
    os.makedirs(final_model_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹æƒé‡
    try:
        trained_model.transformer.save_pretrained(final_model_dir)
    except Exception as e:
        logger.warning(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹æ—¶å‡ºç°è­¦å‘Š: {e}")
        # ä½¿ç”¨torch.saveä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        torch.save(trained_model.state_dict(), os.path.join(final_model_dir, 'pytorch_model.bin'))
        logger.info("ä½¿ç”¨torch.saveä¿å­˜æœ€ç»ˆæ¨¡å‹æƒé‡")
    # ä¿å­˜tokenizer
    tokenizer.save_pretrained(final_model_dir)
    # ä¿å­˜è®­ç»ƒé…ç½®
    try:
        config.save_pretrained(final_model_dir)
    except Exception as e:
        logger.warning(f"ä¿å­˜é…ç½®æ—¶å‡ºç°è­¦å‘Š: {e}")
        # æ‰‹åŠ¨ä¿å­˜é…ç½®
        config_dict = config.to_dict()
        with open(os.path.join(final_model_dir, 'config.json'), 'w') as f:
            json.dump(config_dict, f, indent=2)
        logger.info("æ‰‹åŠ¨ä¿å­˜é…ç½®æ–‡ä»¶")
    
    # æ˜¾ç¤ºè®­ç»ƒå®Œæˆä¿¡æ¯
    console.print(Panel(f"[bold green]ğŸ‰ è®­ç»ƒå®Œæˆï¼[/bold green]\n\n"
                       f"ğŸ“ æœ€ä½³æ¨¡å‹: outputs/best_model/\n"
                       f"ğŸ“ æœ€ç»ˆæ¨¡å‹: {final_model_dir}/\n"
                       f"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ from_pretrained() ç›´æ¥åŠ è½½æ¨¡å‹ï¼\n"
                       f"ğŸš€ è¿è¡Œæ¨ç†: python inference.py --model_path outputs/best_model",
                       title="è®­ç»ƒå®Œæˆ", border_style="green"))
    
    logger.info(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜ä¸ºHuggingFaceæ ‡å‡†æ ¼å¼åˆ° {final_model_dir}")
    logger.info("ç°åœ¨å¯ä»¥ä½¿ç”¨ from_pretrained() ç›´æ¥åŠ è½½æ¨¡å‹ï¼")

if __name__ == "__main__":
    main()
