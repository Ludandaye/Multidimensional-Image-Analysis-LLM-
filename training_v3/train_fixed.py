#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤åçš„è®­ç»ƒè„šæœ¬
è§£å†³æ‰€æœ‰å·²è¯†åˆ«çš„é—®é¢˜ï¼šç‰¹æ®Šç¬¦å·ç»Ÿä¸€ã€ç›‘ç£å¯¹é½ã€æˆªæ–­ç­–ç•¥ã€è¯„ä¼°æ–¹å¼ç­‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import GPT2Config, GPT2LMHeadModel
import os
import logging
import argparse
from tqdm import tqdm
import json

from config.model_config import get_config
from data_processor_fixed import create_datasets
from classification_evaluator import ClassificationEvaluator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FixedGPT2Model(nn.Module):
    """ä¿®å¤åçš„GPT2æ¨¡å‹ï¼Œç¡®ä¿è¯æ±‡è¡¨å¤§å°ä¸€è‡´"""
    
    def __init__(self, config):
        super().__init__()
        
        # åˆ›å»ºGPT2é…ç½®ï¼Œç¡®ä¿è¯æ±‡è¡¨å¤§å°æ­£ç¡®
        gpt2_config = GPT2Config(
            vocab_size=config.model.vocab_size,  # ä½¿ç”¨å®é™…è¯æ±‡è¡¨å¤§å°
            n_positions=config.model.n_positions,
            n_embd=config.model.n_embd,
            n_layer=config.model.n_layer,
            n_head=config.model.n_head,
            bos_token_id=config.vocab.get(config.special_tokens.eos_token, 5),  # æ²¡æœ‰BOSï¼Œç”¨EOS
            eos_token_id=config.vocab.get(config.special_tokens.eos_token, 5),
            pad_token_id=config.vocab.get(config.special_tokens.pad_token, 0)
        )
        
        self.transformer = GPT2LMHeadModel(gpt2_config)
        self.config = config
        
        logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {gpt2_config.vocab_size}")
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        return self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True
        )
    
    def save_pretrained(self, save_path):
        """ä¿å­˜æ¨¡å‹ä¸ºHuggingFaceæ ¼å¼"""
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜transformeréƒ¨åˆ†
        self.transformer.save_pretrained(save_path)
        
        # ä¿å­˜é…ç½®
        config_dict = self.config.model.__dict__.copy()
        config_dict.update({
            'special_tokens': self.config.special_tokens.__dict__,
            'experiment': self.config.experiment.__dict__
        })
        
        with open(os.path.join(save_path, 'unified_config.json'), 'w') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

def train_fixed_model(config, device='cpu', num_epochs=5):
    """ä½¿ç”¨ä¿®å¤åçš„é…ç½®è®­ç»ƒæ¨¡å‹"""
    
    logger.info(f"ğŸš€ å¼€å§‹ä¿®å¤åçš„è®­ç»ƒ...")
    config.print_summary()
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset, val_dataset = create_datasets(config)
    train_loader = DataLoader(train_dataset, batch_size=config.model.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.model.batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = FixedGPT2Model(config)
    model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=config.model.learning_rate)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ClassificationEvaluator(model.transformer, config, device)
    
    best_val_accuracy = 0
    
    for epoch in range(num_epochs):
        logger.info(f"\nğŸ“š ç¬¬ {epoch+1}/{num_epochs} è½®è®­ç»ƒ")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_steps = 0
        
        progress_bar = tqdm(train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}")
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_steps += 1
            
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_train_loss = train_loss / train_steps
        
        # éªŒè¯é˜¶æ®µ - ä½¿ç”¨åˆ†ç±»è¯„ä¼°
        logger.info("ğŸ” è¿›è¡Œåˆ†ç±»è¯„ä¼°...")
        val_results = evaluator.evaluate_dataset(val_dataset, batch_size=8)
        val_accuracy = val_results['accuracy']
        
        logger.info(f"ğŸ“Š ç¬¬{epoch+1}è½®ç»“æœ:")
        logger.info(f"   è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        logger.info(f"   éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2%}")
        logger.info(f"   éªŒè¯ç½®ä¿¡åº¦: {val_results['avg_confidence']:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            save_path = 'outputs/best_model_fixed'
            model.save_pretrained(save_path)
            
            # ä¿å­˜è¯æ±‡è¡¨
            import shutil
            shutil.copy(config.data.vocab_path, os.path.join(save_path, 'vocab.json'))
            
            logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å‡†ç¡®ç‡: {val_accuracy:.2%}")
    
    # æœ€ç»ˆè¯„ä¼°
    logger.info(f"\nğŸ† æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    logger.info(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2%}")
    
    return best_val_accuracy

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--device', type=str, default='auto', help='è®¾å¤‡')
    args = parser.parse_args()
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è·å–é…ç½®
    config = get_config()
    
    # è®­ç»ƒæ¨¡å‹
    best_accuracy = train_fixed_model(config, device, args.epochs)
    
    print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2%}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: outputs/best_model_fixed")
