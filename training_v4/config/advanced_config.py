#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Training V4 é«˜çº§é…ç½®ç®¡ç†ç³»ç»Ÿ
æ•´åˆv3çš„æ‰€æœ‰æ”¹è¿›ï¼Œå¢åŠ è‡ªåŠ¨è°ƒä¼˜å’Œæ™ºèƒ½é…ç½®
"""

import json
import os
import math
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict, field
from pathlib import Path
from datetime import datetime
import torch

@dataclass
class PerformanceConfig:
    """æ€§èƒ½ä¼˜åŒ–é…ç½®"""
    # GPUä¼˜åŒ–
    mixed_precision: bool = True
    gradient_accumulation_steps: int = 1
    dataloader_num_workers: int = 4
    pin_memory: bool = True
    
    # å†…å­˜ä¼˜åŒ–
    max_memory_fraction: float = 0.85
    empty_cache_steps: int = 100
    checkpoint_gradient: bool = True
    
    # è®­ç»ƒä¼˜åŒ–
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    gradient_clip_norm: float = 1.0
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler_type: str = "cosine_with_restarts"  # cosine, cosine_with_restarts, polynomial
    lr_restart_cycles: int = 3
    lr_decay_factor: float = 0.8

@dataclass
class DataConfig:
    """æ•°æ®é…ç½® - V4å¢å¼ºç‰ˆ"""
    # åŸºç¡€é…ç½®
    train_data_path: str = "../training_v3/generated_sequences_super_enhanced/sequences_labels_fixed_v2_maxlen1024.jsonl"
    vocab_path: str = "../training_v3/outputs/best_model_silent/vocab.json"
    
    # åºåˆ—é…ç½®
    max_length: int = 1024
    adaptive_length: bool = True  # è‡ªé€‚åº”åºåˆ—é•¿åº¦
    padding_strategy: str = "max_length"  # max_length, longest, do_not_pad
    
    # æ•°æ®å¢å¼º
    enable_augmentation: bool = True
    augmentation_ratio: float = 0.2
    noise_ratio: float = 0.05
    
    # æ•°æ®åˆ’åˆ†
    train_ratio: float = 0.8
    val_ratio: float = 0.15
    test_ratio: float = 0.05
    random_seed: int = 42
    
    # ç¼“å­˜é…ç½®
    enable_caching: bool = True
    cache_dir: str = "data/cache"
    preload_data: bool = True

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½® - V4æ™ºèƒ½ç‰ˆ"""
    # åŸºç¡€æ¶æ„
    model_type: str = "gpt2"
    vocab_size: int = 516
    
    # å¯è°ƒå‚æ•° - æ”¯æŒè‡ªåŠ¨è°ƒä¼˜
    n_embd: int = 768
    n_layer: int = 12
    n_head: int = 16
    n_positions: int = 1024
    
    # é«˜çº§é…ç½®
    activation_function: str = "gelu_new"
    layer_norm_epsilon: float = 1e-5
    initializer_range: float = 0.02
    
    # Dropouté…ç½®
    resid_pdrop: float = 0.1
    embd_pdrop: float = 0.1
    attn_pdrop: float = 0.1
    summary_first_dropout: float = 0.1
    
    # ç‰¹æ®Šé…ç½®
    use_cache: bool = True
    scale_attn_weights: bool = True
    reorder_and_upcast_attn: bool = False
    
    # è‡ªåŠ¨è°ƒä¼˜ç›¸å…³
    auto_scale_for_gpu: bool = True
    target_gpu_utilization: float = 0.85
    min_batch_size: int = 2
    max_batch_size: int = 32

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½® - V4è‡ªé€‚åº”ç‰ˆ"""
    # åŸºç¡€è®­ç»ƒå‚æ•°
    num_epochs: int = 500
    batch_size: int = 8
    learning_rate: float = 3e-4
    
    # è‡ªé€‚åº”é…ç½®
    adaptive_batch_size: bool = True
    adaptive_learning_rate: bool = True
    early_stopping: bool = True
    patience: int = 20
    
    # å­¦ä¹ ç‡ç­–ç•¥
    warmup_steps: int = 500
    min_learning_rate: float = 1e-6
    lr_schedule_patience: int = 10
    lr_schedule_factor: float = 0.5
    
    # éªŒè¯å’Œä¿å­˜
    eval_steps: int = 100
    save_steps: int = 500
    logging_steps: int = 50
    save_total_limit: int = 5
    
    # å®éªŒè¿½è¸ª
    experiment_name: str = "training_v4_advanced"
    run_name: str = None  # è‡ªåŠ¨ç”Ÿæˆ
    track_metrics: List[str] = field(default_factory=lambda: ["accuracy", "loss", "perplexity", "lr"])
    
    # æ¢å¤è®­ç»ƒ
    resume_from_checkpoint: bool = True
    checkpoint_dir: str = "outputs/checkpoints"

@dataclass
class MonitoringConfig:
    """ç›‘æ§é…ç½® - V4å…¨é¢ç›‘æ§"""
    # åŸºç¡€ç›‘æ§
    enable_wandb: bool = False  # å¯é€‰ï¼šWeights & Biases
    enable_tensorboard: bool = True
    enable_mlflow: bool = False
    
    # GPUç›‘æ§
    monitor_gpu: bool = True
    gpu_log_interval: int = 30  # ç§’
    
    # æ€§èƒ½ç›‘æ§
    monitor_memory: bool = True
    monitor_throughput: bool = True
    profile_training: bool = False
    
    # é¢„è­¦ç³»ç»Ÿ
    enable_alerts: bool = True
    accuracy_threshold: float = 0.15  # å‡†ç¡®ç‡é˜ˆå€¼
    loss_spike_threshold: float = 2.0  # æŸå¤±çªå¢é˜ˆå€¼
    
    # æ—¥å¿—é…ç½®
    log_level: str = "INFO"
    save_detailed_logs: bool = True
    log_predictions: bool = True
    max_prediction_samples: int = 100

@dataclass
class SpecialTokensConfig:
    """ç‰¹æ®ŠTokené…ç½® - V4æ ‡å‡†åŒ–"""
    # åŸºç¡€special tokens
    pad_token: str = "<PAD>"
    unk_token: str = "<UNK>"
    eos_token: str = "<EOS>"
    bos_token: str = "<BOS>"  # V4æ–°å¢
    
    # å›¾åƒç›¸å…³
    img_start_token: str = "<IMG>"
    img_end_token: str = "</IMG>"
    
    # åˆ†ç±»ç›¸å…³
    cls_token: str = "<CLS>"
    cls_tokens: List[str] = field(default_factory=lambda: [f"<CLS_{i}>" for i in range(10)])
    
    # V4æ–°å¢ï¼šè¯­ä¹‰æ ‡è®°
    sep_token: str = "<SEP>"  # åˆ†éš”ç¬¦
    mask_token: str = "<MASK>"  # æ©ç token
    
    # Token IDs (è‡ªåŠ¨ä»vocabåŠ è½½)
    token_ids: Dict[str, int] = field(default_factory=dict)

class AdvancedConfigManager:
    """V4é«˜çº§é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str = "."):
        self.base_dir = Path(base_dir)
        self.config_file = self.base_dir / "config" / "v4_config.json"
        
        # åˆå§‹åŒ–å„æ¨¡å—é…ç½®
        self.model = ModelConfig()
        self.training = TrainingConfig()
        self.data = DataConfig()
        self.performance = PerformanceConfig()
        self.monitoring = MonitoringConfig()
        self.special_tokens = SpecialTokensConfig()
        
        # ç³»ç»Ÿä¿¡æ¯
        self.system_info = self._detect_system()
        self.gpu_info = self._detect_gpu()
        
        # åŠ è½½è¯æ±‡è¡¨å’Œè‡ªåŠ¨é…ç½®
        self.vocab = self._load_vocab()
        self._setup_special_tokens()
        self._auto_configure()
        
        print(f"âœ… V4é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ¯ ç³»ç»Ÿ: {self.system_info['platform']}")
        print(f"ğŸš€ GPU: {self.gpu_info['name']} ({self.gpu_info['memory_gb']:.1f}GB)")
    
    def _detect_system(self) -> Dict[str, Any]:
        """æ£€æµ‹ç³»ç»Ÿä¿¡æ¯"""
        import platform
        import psutil
        
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "architecture": platform.architecture()[0]
        }
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        if not torch.cuda.is_available():
            return {"available": False, "name": "CPU", "memory_gb": 0}
        
        gpu_props = torch.cuda.get_device_properties(0)
        return {
            "available": True,
            "name": gpu_props.name,
            "memory_gb": gpu_props.total_memory / (1024**3),
            "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
            "multiprocessor_count": getattr(gpu_props, 'multiprocessor_count', 'unknown')
        }
    
    def _load_vocab(self) -> Dict[str, int]:
        """åŠ è½½è¯æ±‡è¡¨"""
        vocab_path = Path(self.data.vocab_path)
        if vocab_path.exists():
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab = json.load(f)
                print(f"âœ… ä» {vocab_path} åŠ è½½è¯æ±‡è¡¨ ({len(vocab)} tokens)")
                return vocab
        else:
            print(f"âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}")
            return self._create_default_vocab()
    
    def _create_default_vocab(self) -> Dict[str, int]:
        """åˆ›å»ºé»˜è®¤è¯æ±‡è¡¨"""
        vocab = {}
        
        # ç‰¹æ®Štokens
        vocab['<PAD>'] = 0
        vocab['<UNK>'] = 1
        vocab['<IMG>'] = 2
        vocab['</IMG>'] = 3
        vocab['<CLS>'] = 4
        vocab['<EOS>'] = 5
        vocab['<BOS>'] = 6  # V4æ–°å¢
        vocab['<SEP>'] = 7  # V4æ–°å¢
        vocab['<MASK>'] = 8  # V4æ–°å¢
        
        # å›¾åƒtokens
        for i in range(500):
            vocab[f'<Z_{i:03d}>'] = 9 + i
        
        # åˆ†ç±»tokens
        for i in range(10):
            vocab[f'<CLS_{i}>'] = 509 + i
        
        print(f"ğŸ“ åˆ›å»ºé»˜è®¤è¯æ±‡è¡¨ ({len(vocab)} tokens)")
        return vocab
    
    def _setup_special_tokens(self):
        """è®¾ç½®ç‰¹æ®Štokençš„IDæ˜ å°„"""
        for token_name in ['pad_token', 'unk_token', 'eos_token', 'bos_token', 
                          'img_start_token', 'img_end_token', 'cls_token', 'sep_token', 'mask_token']:
            token_value = getattr(self.special_tokens, token_name)
            if token_value in self.vocab:
                self.special_tokens.token_ids[token_name] = self.vocab[token_value]
        
        # åˆ†ç±»tokens
        for i, cls_token in enumerate(self.special_tokens.cls_tokens):
            if cls_token in self.vocab:
                self.special_tokens.token_ids[f'cls_{i}'] = self.vocab[cls_token]
    
    def _auto_configure(self):
        """æ ¹æ®ç³»ç»Ÿä¿¡æ¯è‡ªåŠ¨é…ç½®"""
        if self.model.auto_scale_for_gpu and self.gpu_info['available']:
            self._auto_scale_model()
        
        if self.training.adaptive_batch_size:
            self._auto_adjust_batch_size()
        
        # ç”Ÿæˆè¿è¡Œåç§°
        if not self.training.run_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.training.run_name = f"v4_{timestamp}_gpu{int(self.gpu_info['memory_gb'])}gb"
    
    def _auto_scale_model(self):
        """æ ¹æ®GPUè‡ªåŠ¨è°ƒæ•´æ¨¡å‹è§„æ¨¡"""
        gpu_memory = self.gpu_info['memory_gb']
        
        if gpu_memory >= 80:  # A100 ç­‰é«˜ç«¯å¡
            self.model.n_embd = 1024
            self.model.n_layer = 16
            self.model.n_head = 16
            self.training.batch_size = 16
            print("ğŸš€ æ£€æµ‹åˆ°é«˜ç«¯GPUï¼Œä½¿ç”¨Largeé…ç½®")
        elif gpu_memory >= 24:  # RTX 4090ç­‰
            self.model.n_embd = 768
            self.model.n_layer = 12
            self.model.n_head = 16
            self.training.batch_size = 12
            print("ğŸ’ª æ£€æµ‹åˆ°é«˜æ€§èƒ½GPUï¼Œä½¿ç”¨Mediumé…ç½®")
        elif gpu_memory >= 12:  # RTX 3080ç­‰
            self.model.n_embd = 512
            self.model.n_layer = 8
            self.model.n_head = 8
            self.training.batch_size = 8
            print("âš¡ æ£€æµ‹åˆ°ä¸­ç«¯GPUï¼Œä½¿ç”¨Smallé…ç½®")
        else:  # ä½ç«¯GPU
            self.model.n_embd = 384
            self.model.n_layer = 6
            self.model.n_head = 6
            self.training.batch_size = 4
            print("ğŸ”§ æ£€æµ‹åˆ°å…¥é—¨GPUï¼Œä½¿ç”¨Tinyé…ç½®")
    
    def _auto_adjust_batch_size(self):
        """æ ¹æ®æ¨¡å‹å¤§å°è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        # ä¼°ç®—å‚æ•°é‡
        param_count = self._estimate_parameters()
        param_millions = param_count / 1_000_000
        
        # æ ¹æ®å‚æ•°é‡å’ŒGPUå†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if param_millions > 500:  # å¤§æ¨¡å‹
            self.training.batch_size = max(2, min(8, self.training.batch_size))
        elif param_millions > 200:  # ä¸­æ¨¡å‹
            self.training.batch_size = max(4, min(16, self.training.batch_size))
        else:  # å°æ¨¡å‹
            self.training.batch_size = max(8, min(32, self.training.batch_size))
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {param_millions:.1f}M, è°ƒæ•´æ‰¹æ¬¡å¤§å°: {self.training.batch_size}")
    
    def _estimate_parameters(self) -> int:
        """ä¼°ç®—æ¨¡å‹å‚æ•°é‡"""
        vocab_size = self.model.vocab_size
        n_embd = self.model.n_embd
        n_layer = self.model.n_layer
        n_positions = self.model.n_positions
        
        # åµŒå…¥å±‚å‚æ•°
        emb_params = vocab_size * n_embd + n_positions * n_embd
        
        # Transformerå±‚å‚æ•°
        attn_params = 4 * n_embd * n_embd  # Q,K,V,O
        ffn_params = 8 * n_embd * n_embd   # FFN (4x expansion)
        norm_params = 2 * n_embd           # LayerNorm
        
        layer_params = (attn_params + ffn_params + norm_params) * n_layer
        
        # è¾“å‡ºå±‚
        output_params = vocab_size * n_embd
        
        return emb_params + layer_params + output_params
    
    def get_model_config_dict(self) -> Dict:
        """è·å–HuggingFaceå…¼å®¹çš„æ¨¡å‹é…ç½®"""
        return {
            "architectures": ["GPT2LMHeadModel"],
            "model_type": self.model.model_type,
            "vocab_size": self.model.vocab_size,
            "n_positions": self.model.n_positions,
            "n_embd": self.model.n_embd,
            "n_layer": self.model.n_layer,
            "n_head": self.model.n_head,
            "activation_function": self.model.activation_function,
            "resid_pdrop": self.model.resid_pdrop,
            "embd_pdrop": self.model.embd_pdrop,
            "attn_pdrop": self.model.attn_pdrop,
            "layer_norm_epsilon": self.model.layer_norm_epsilon,
            "initializer_range": self.model.initializer_range,
            "summary_type": "cls_index",
            "summary_use_proj": True,
            "summary_activation": None,
            "summary_proj_to_labels": True,
            "summary_first_dropout": self.model.summary_first_dropout,
            "scale_attn_weights": self.model.scale_attn_weights,
            "use_cache": self.model.use_cache,
            "pad_token_id": self.special_tokens.token_ids.get('pad_token', 0),
            "eos_token_id": self.special_tokens.token_ids.get('eos_token', 5),
            "bos_token_id": self.special_tokens.token_ids.get('bos_token', 6),
        }
    
    def save_config(self, save_path: Optional[str] = None):
        """ä¿å­˜å®Œæ•´é…ç½®"""
        if save_path is None:
            save_path = self.config_file
        
        config_dict = {
            "model": asdict(self.model),
            "training": asdict(self.training),
            "data": asdict(self.data),
            "performance": asdict(self.performance),
            "monitoring": asdict(self.monitoring),
            "special_tokens": asdict(self.special_tokens),
            "system_info": self.system_info,
            "gpu_info": self.gpu_info,
            "vocab_size": len(self.vocab),
            "estimated_parameters": self._estimate_parameters(),
            "created_at": datetime.now().isoformat()
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_config(self, config_path: str):
        """åŠ è½½é…ç½®"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        
        self.model = ModelConfig(**config_dict['model'])
        self.training = TrainingConfig(**config_dict['training'])
        self.data = DataConfig(**config_dict['data'])
        self.performance = PerformanceConfig(**config_dict['performance'])
        self.monitoring = MonitoringConfig(**config_dict['monitoring'])
        self.special_tokens = SpecialTokensConfig(**config_dict['special_tokens'])
        
        print(f"ğŸ“‚ é…ç½®å·²åŠ è½½: {config_path}")
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*80)
        print(f"ğŸš€ Training V4 é…ç½®æ‘˜è¦ - {self.training.experiment_name}")
        print("="*80)
        
        print(f"ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯:")
        print(f"   - å¹³å°: {self.system_info['platform']}")
        print(f"   - GPU: {self.gpu_info['name']} ({self.gpu_info['memory_gb']:.1f}GB)")
        print(f"   - å†…å­˜: {self.system_info['memory_gb']:.1f}GB")
        
        print(f"ğŸ§  æ¨¡å‹é…ç½®:")
        print(f"   - æ¶æ„: {self.model.n_embd}d-{self.model.n_layer}å±‚-{self.model.n_head}å¤´")
        print(f"   - åºåˆ—é•¿åº¦: {self.model.n_positions}")
        print(f"   - è¯æ±‡è¡¨: {self.model.vocab_size} tokens")
        print(f"   - ä¼°ç®—å‚æ•°: {self._estimate_parameters()/1_000_000:.1f}M")
        
        print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
        print(f"   - è½®æ•°: {self.training.num_epochs}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {self.training.batch_size}")
        print(f"   - å­¦ä¹ ç‡: {self.training.learning_rate}")
        print(f"   - è¿è¡Œåç§°: {self.training.run_name}")
        
        print(f"ğŸ“Š æ€§èƒ½ä¼˜åŒ–:")
        print(f"   - æ··åˆç²¾åº¦: {self.performance.mixed_precision}")
        print(f"   - æ¢¯åº¦ç´¯ç§¯: {self.performance.gradient_accumulation_steps}")
        print(f"   - è°ƒåº¦å™¨: {self.performance.scheduler_type}")
        
        print(f"ğŸ“ˆ ç›‘æ§é…ç½®:")
        print(f"   - GPUç›‘æ§: {self.monitoring.monitor_gpu}")
        print(f"   - TensorBoard: {self.monitoring.enable_tensorboard}")
        print(f"   - è¯¦ç»†æ—¥å¿—: {self.monitoring.save_detailed_logs}")

# ä¾¿æ·å‡½æ•°
def get_v4_config(config_name: str = "auto") -> AdvancedConfigManager:
    """è·å–V4é…ç½®"""
    manager = AdvancedConfigManager()
    
    if config_name != "auto":
        # é¢„å®šä¹‰é…ç½®
        configs = {
            "tiny": {"n_embd": 256, "n_layer": 4, "n_head": 4},
            "small": {"n_embd": 384, "n_layer": 6, "n_head": 6},
            "medium": {"n_embd": 768, "n_layer": 12, "n_head": 16},
            "large": {"n_embd": 1024, "n_layer": 16, "n_head": 16},
            "xlarge": {"n_embd": 1536, "n_layer": 20, "n_head": 24}
        }
        
        if config_name in configs:
            config = configs[config_name]
            manager.model.n_embd = config["n_embd"]
            manager.model.n_layer = config["n_layer"]
            manager.model.n_head = config["n_head"]
            print(f"ğŸ¯ ä½¿ç”¨é¢„å®šä¹‰é…ç½®: {config_name}")
    
    return manager

if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç®¡ç†å™¨
    print("ğŸ§ª æµ‹è¯•V4é…ç½®ç®¡ç†å™¨")
    
    config = get_v4_config()
    config.print_summary()
    config.save_config()
    
    print("\nâœ… V4é…ç½®ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
