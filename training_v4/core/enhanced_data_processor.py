#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Training V4 å¢å¼ºæ•°æ®å¤„ç†å™¨
æŒ‰ç…§ç”¨æˆ·è¦æ±‚ï¼š1024å…¨å±€é•¿åº¦ï¼Œç¡®ä¿CLS tokenä¸è¢«æˆªæ–­
"""

import json
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import logging

class EnhancedDataProcessor:
    """
    å¢å¼ºæ•°æ®å¤„ç†å™¨ - V4ä¸“ç”¨
    ç¡®ä¿è®­ç»ƒæ•°æ®ç¬¦åˆæ‰€æœ‰è¦æ±‚
    """

    def __init__(self, vocab: Dict[str, int], max_length: int = 1024):
        self.vocab = vocab
        self.max_length = max_length
        # ç‰¹æ®Štoken IDs
        self.pad_token_id = vocab.get('<PAD>', 0)
        self.cls_token_id = vocab.get('<CLS>', 4)
        self.eos_token_id = vocab.get('<EOS>', 5)
        # åˆ†ç±»tokenæ˜ å°„
        self.cls_tokens = {i: vocab.get(f'<CLS_{i}>', 509 + i) for i in range(10)}
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ğŸ“Š æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–: max_length={max_length}")
        self.logger.info(f"ğŸ¯ CLS tokenæ˜ å°„: {self.cls_tokens}")

    def process_sequence(self, token_ids: List[int], label: int) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªåºåˆ— - ç¡®ä¿CLS tokenå®Œæ•´
        """
        # éªŒè¯æ ‡ç­¾
        if not (0 <= label <= 9):
            self.logger.warning(f"æ ‡ç­¾{label}è¶…å‡ºèŒƒå›´[0,9]ï¼Œè·³è¿‡")
            return None

        # å·¦æˆªæ–­ç­–ç•¥ï¼šä¿æŠ¤å°¾éƒ¨CLS token
        if len(token_ids) > self.max_length:
            # ä¿ç•™æœ€åmax_lengthä¸ªtokenï¼Œä¿æŠ¤CLSå’ŒEOS
            token_ids = token_ids[-self.max_length:]
            self.logger.debug(f"åºåˆ—æˆªæ–­: åŸé•¿åº¦ -> {self.max_length}")

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(token_ids) < self.max_length:
            padding_length = self.max_length - len(token_ids)
            token_ids = token_ids + [self.pad_token_id] * padding_length

        # æŸ¥æ‰¾CLS tokenä½ç½®
        cls_position = -1
        for i, token_id in enumerate(token_ids):
            if token_id == self.cls_token_id:
                cls_position = i
                break

        if cls_position == -1:
            self.logger.warning("æœªæ‰¾åˆ°<CLS> tokenï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return None

        # è·å–ç›®æ ‡CLS_X token
        cls_target_token = self.cls_tokens.get(label)
        if cls_target_token is None:
            self.logger.error(f"æ‰¾ä¸åˆ°æ ‡ç­¾{label}å¯¹åº”çš„CLS token")
            return None

        # åˆ›å»ºattention mask
        attention_mask = [1 if t != self.pad_token_id else 0 for t in token_ids]

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not self._validate_sample(token_ids, cls_position, label, cls_target_token):
            return None

        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'labels': torch.tensor(token_ids, dtype=torch.long),  # åºåˆ—å»ºæ¨¡ç›®æ ‡
            'cls_position': cls_position,
            'cls_label': label,
            'cls_target_token': cls_target_token
        }

    def _validate_sample(self, token_ids: List[int], cls_position: int, label: int, cls_target: int) -> bool:
        """éªŒè¯æ ·æœ¬å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥åºåˆ—é•¿åº¦
            if len(token_ids) != self.max_length:
                self.logger.error(f"åºåˆ—é•¿åº¦é”™è¯¯: {len(token_ids)} != {self.max_length}")
                return False

            # æ£€æŸ¥CLSä½ç½®
            if cls_position < 0 or cls_position >= len(token_ids):
                self.logger.error(f"CLSä½ç½®æ— æ•ˆ: {cls_position}")
                return False

            # éªŒè¯CLS token
            if token_ids[cls_position] != self.cls_token_id:
                self.logger.error(f"CLSä½ç½®tokené”™è¯¯: {token_ids[cls_position]} != {self.cls_token_id}")
                return False

            # æ£€æŸ¥æ ‡ç­¾å’Œç›®æ ‡tokenä¸€è‡´æ€§
            expected_target = self.cls_tokens.get(label)
            if cls_target != expected_target:
                self.logger.error(f"ç›®æ ‡tokenä¸ä¸€è‡´: {cls_target} != {expected_target}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"æ ·æœ¬éªŒè¯å¼‚å¸¸: {e}")
            return False

    def load_and_process_data(self, data_file: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½å¹¶å¤„ç†æ•°æ®æ–‡ä»¶
        """
        self.logger.info(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")

        if not Path(data_file).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

        processed_data: List[Dict[str, Any]] = []
        total_lines = 0
        valid_samples = 0
        cls_found_count = 0

        with open(data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                total_lines += 1

                try:
                    item = json.loads(line.strip())

                    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                    if 'input_ids' in item:
                        token_ids = item['input_ids']
                    elif 'tokens' in item:
                        # å¤„ç†tokenså­—ç¬¦ä¸²æ ¼å¼
                        tokens_str = item['tokens']
                        if isinstance(tokens_str, str):
                            token_names = tokens_str.split()
                            token_ids = [self.vocab.get(token, self.vocab.get('<UNK>', 1)) for token in token_names]
                        else:
                            token_ids = tokens_str
                    else:
                        continue

                    label = item.get('label', -1)

                    # å¤„ç†åºåˆ—
                    processed_item = self.process_sequence(token_ids, label)

                    if processed_item is not None:
                        processed_data.append(processed_item)
                        valid_samples += 1
                        cls_found_count += 1

                    # è¿›åº¦æŠ¥å‘Š
                    if (line_idx + 1) % 100 == 0:
                        self.logger.info(f"å¤„ç†è¿›åº¦: {line_idx + 1}/{total_lines} è¡Œï¼Œæœ‰æ•ˆ: {valid_samples}")

                except json.JSONDecodeError as e:
                    self.logger.error(f"JSONè§£æé”™è¯¯ ç¬¬{line_idx + 1}è¡Œ: {e}")
                    continue
                except Exception as e:
                    self.logger.error(f"å¤„ç†é”™è¯¯ ç¬¬{line_idx + 1}è¡Œ: {e}")
                    continue

        # æ•°æ®ç»Ÿè®¡
        self.logger.info(f"ğŸ“Š æ•°æ®å¤„ç†å®Œæˆ:")
        self.logger.info(f"   - æ€»è¡Œæ•°: {total_lines}")
        self.logger.info(f"   - æœ‰æ•ˆæ ·æœ¬: {valid_samples}")
        if total_lines:
            self.logger.info(f"   - æˆåŠŸç‡: {valid_samples/total_lines*100:.2f}%")
            self.logger.info(f"   - CLSæ‰¾åˆ°ç‡: {cls_found_count/total_lines*100:.2f}%")

        if valid_samples == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")

        return processed_data

class ImageClassificationDataset(Dataset):
    """å›¾åƒåˆ†ç±»æ•°æ®é›† - V4ä¸“ç”¨"""
    
    def __init__(self, processed_data: List[Dict[str, Any]]):
        self.data = processed_data
        print(f"ğŸ“Š æ•°æ®é›†åˆå§‹åŒ–: {len(self.data)} æ ·æœ¬")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]
