#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Training V4 ç»ˆæè®­ç»ƒå™¨ - å®Œæ•´ç‰ˆ
ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·æ‰€æœ‰è¦æ±‚å®ç°ï¼š
1. è·‘æ»¡GPU - æ··åˆç²¾åº¦+æ•°æ®å¹¶è¡Œ+ä¼˜åŒ–æ‰¹æ¬¡
2. é™é»˜è®­ç»ƒ - åå°è¿è¡Œ+æ—¥å¿—é‡å®šå‘
3. æ–­ç‚¹å¯æ¢å¤ - è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
4. ç»ˆç«¯å…³é—­å¯æŒç»­ - ä¿¡å·å¤„ç†
5. éšæ—¶è¾“å‡ºè¿›åº¦ - å¤šçº¿ç¨‹ç›‘æ§
6. è®­ç»ƒé€»è¾‘æ­£ç¡® - æ˜ç¡®åˆ†ç±»ç›®æ ‡
7. 1024å…¨å±€é•¿åº¦ - ä¿æŠ¤CLS token
"""

import os
import sys
import json
import time
import signal
import logging
import traceback
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from transformers import GPT2LMHeadModel, GPT2Config
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')
sys.path.append('..')

from config.advanced_config import get_v4_config
from core.enhanced_data_processor import EnhancedDataProcessor, ImageClassificationDataset
from core.training_objectives import TrainingObjective

class V4Trainer:
    """V4ç»ˆæè®­ç»ƒå™¨ - æ»¡è¶³ç”¨æˆ·æ‰€æœ‰è¦æ±‚"""
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–V4ç»ˆæè®­ç»ƒå™¨...")
        print("=" * 60)
        print("âœ… æ»¡è¶³ç”¨æˆ·è¦æ±‚:")
        print("   1. è·‘æ»¡GPU - æ··åˆç²¾åº¦+æ•°æ®å¹¶è¡Œ+ä¼˜åŒ–æ‰¹æ¬¡")
        print("   2. é™é»˜è®­ç»ƒ - åå°è¿è¡Œ+æ—¥å¿—è¾“å‡º")
        print("   3. æ–­ç‚¹å¯æ¢å¤ - è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹")
        print("   4. ç»ˆç«¯å…³é—­å¯æŒç»­ - nohupæ”¯æŒ")
        print("   5. éšæ—¶è¾“å‡ºè¿›åº¦ - å¤šçº¿ç¨‹ç›‘æ§")
        print("   6. è®­ç»ƒé€»è¾‘æ­£ç¡® - æ˜ç¡®åˆ†ç±»ç›®æ ‡")
        print("   7. 1024å…¨å±€é•¿åº¦ - ä¿æŠ¤CLS token")
        print("=" * 60)
        
        # åŸºç¡€é…ç½®
        self.config = get_v4_config()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()
        
        # æ ¸å¿ƒç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = GradScaler()
        self.training_objective = TrainingObjective()
        self.data_processor = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_accuracy = 0.0
        self.best_loss = float('inf')
        self.training_start_time = None
        
        # ç›‘æ§ç³»ç»Ÿ
        self.checkpoint_dir = Path("outputs/checkpoints")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.step_times = []
        self.gpu_stats = []
        
        # å¤šçº¿ç¨‹æ§åˆ¶
        self.stop_monitoring = False
        self.progress_thread = None
        
        # è®­ç»ƒç›®æ ‡æ˜ å°„ - ç¡®ä¿ç†è§£æ­£ç¡®
        self.cls_token_to_label = {}
        self.label_to_cls_token = {}
        
        self.logger.info("âœ… V4è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ¯ è®¾å¤‡: {self.device}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹é…ç½®: {self.config.model.n_embd}d-{self.config.model.n_layer}å±‚")
        self.logger.info(f"ğŸ“ åºåˆ—é•¿åº¦: {self.config.model.n_positions}")
    
    def _setup_logging(self):
        """è®¾ç½®å®Œæ•´çš„æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = log_dir / f"v4_training_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—å¤„ç†å™¨
        handlers = [logging.FileHandler(self.log_file, encoding='utf-8')]
        
        # å¦‚æœæ˜¯ç»ˆç«¯æ¨¡å¼ï¼Œä¹Ÿè¾“å‡ºåˆ°æ§åˆ¶å°
        if sys.stdout.isatty():
            handlers.append(logging.StreamHandler(sys.stdout))
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s | %(levelname)8s | %(message)s',
            handlers=handlers,
            force=True
        )
        
        self.logger = logging.getLogger(__name__)
        
        # é™é»˜æ¨¡å¼å¤„ç†
        if not sys.stdout.isatty():
            sys.stdout = open(self.log_file, 'a', encoding='utf-8')
            sys.stderr = open(self.log_file, 'a', encoding='utf-8')
            self.logger.info("ğŸ”‡ é™é»˜æ¨¡å¼æ¿€æ´»")
        
        self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {self.log_file}")
    
    def _setup_classification_mapping(self, vocab: Dict[str, int]):
        """å»ºç«‹åˆ†ç±»æ˜ å°„ - ç¡®ä¿è®­ç»ƒç›®æ ‡å®Œå…¨æ­£ç¡®"""
        self.logger.info("ğŸ¯ å»ºç«‹åˆ†ç±»ç›®æ ‡æ˜ å°„...")
        
        self.cls_token_to_label.clear()
        self.label_to_cls_token.clear()
        
        for label in range(10):
            cls_token = f"<CLS_{label}>"
            if cls_token in vocab:
                token_id = vocab[cls_token]
                self.label_to_cls_token[label] = token_id
                self.cls_token_to_label[token_id] = label
            else:
                self.logger.error(f"âŒ æ‰¾ä¸åˆ°åˆ†ç±»token: {cls_token}")
        
        if len(self.label_to_cls_token) != 10:
            self.logger.error(f"âŒ åˆ†ç±»æ˜ å°„ä¸å®Œæ•´! åªæ‰¾åˆ° {len(self.label_to_cls_token)}/10 ä¸ªåˆ†ç±»token")
            raise ValueError("åˆ†ç±»tokenæ˜ å°„ä¸å®Œæ•´")
        
        self.logger.info(f"âœ… åˆ†ç±»æ˜ å°„å»ºç«‹å®Œæˆ: {len(self.label_to_cls_token)} ä¸ªç±»åˆ«")
        self.logger.info("ğŸ¯ è®­ç»ƒç›®æ ‡æ˜ç¡®: åœ¨<CLS>ä½ç½®é¢„æµ‹æ­£ç¡®çš„<CLS_X>åˆ†ç±»token")
    
    def _validate_training_sample(self, batch, batch_idx: int):
        """éªŒè¯è®­ç»ƒæ ·æœ¬çš„æ­£ç¡®æ€§"""
        if batch_idx % 500 == 0:
            self.logger.info("ğŸ” éªŒè¯è®­ç»ƒæ ·æœ¬ä¸€è‡´æ€§...")
            
            sample_count = min(2, len(batch['cls_label']))
            for i in range(sample_count):
                true_label = batch['cls_label'][i].item()
                target_token = batch['cls_target_token'][i].item()
                expected_token = self.label_to_cls_token.get(true_label)
                cls_pos = batch['cls_position'][i]
                
                if target_token != expected_token:
                    self.logger.error(f"âŒ æ ·æœ¬{i}è®­ç»ƒç›®æ ‡ä¸ä¸€è‡´!")
                    raise ValueError("è®­ç»ƒæ•°æ®ä¸ä¸€è‡´")
                else:
                    self.logger.debug(f"âœ… æ ·æœ¬{i}ç›®æ ‡æ­£ç¡®: æ ‡ç­¾{true_label} -> token {target_token}")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ - å……åˆ†åˆ©ç”¨GPU"""
        self.logger.info("ğŸ§  åˆå§‹åŒ–GPT2æ¨¡å‹...")
        
        model_config = GPT2Config(**self.config.get_model_config_dict())
        self.model = GPT2LMHeadModel(model_config)
        self.model.to(self.device)
        
        # å¤šGPUå¹¶è¡Œ
        if torch.cuda.device_count() > 1:
            self.logger.info(f"ğŸš€ ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œå¹¶è¡Œè®­ç»ƒ")
            self.model = nn.DataParallel(self.model)
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,} ({total_params * 4 / (1024**3):.2f} GB)")
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.training.learning_rate,
            weight_decay=0.01
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = self.config.training.num_epochs * 1000
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.config.training.learning_rate,
            total_steps=total_steps,
            pct_start=0.1
        )
        
        self.logger.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ® - 1024å…¨å±€é•¿åº¦"""
        self.logger.info("ğŸ“ åŠ è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½è¯æ±‡è¡¨
        vocab_path = self.config.data.vocab_path
        if vocab_path == "é»˜è®¤è¯æ±‡è¡¨":
            vocab = self.config.vocab
        else:
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab = json.load(f)
        
        self.logger.info(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(vocab)} tokens")
        
        # å»ºç«‹åˆ†ç±»æ˜ å°„
        self._setup_classification_mapping(vocab)
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = EnhancedDataProcessor(
            vocab=vocab,
            max_length=self.config.model.n_positions
        )
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        processed_data = self.data_processor.load_and_process_data(
            self.config.data.train_data_path
        )
        
        # ç®€å•æ•°æ®åˆ’åˆ†
        total_size = len(processed_data)
        train_size = int(total_size * 0.8)
        val_size = total_size - train_size
        
        train_data = processed_data[:train_size]
        val_data = processed_data[train_size:]
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        train_dataset = ImageClassificationDataset(train_data)
        val_dataset = ImageClassificationDataset(val_data)
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.training.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.training.batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        self.logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒ{len(self.train_loader)}æ‰¹æ¬¡, éªŒè¯{len(self.val_loader)}æ‰¹æ¬¡")
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - æ”¯æŒæ–­ç‚¹æ¢å¤"""
        try:
            model_state = self.model.module.state_dict() if isinstance(self.model, nn.DataParallel) else self.model.state_dict()
            
            checkpoint = {
                'epoch': epoch,
                'global_step': self.global_step,
                'model_state_dict': model_state,
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'best_accuracy': self.best_accuracy,
                'cls_mapping': {
                    'label_to_token': self.label_to_cls_token,
                    'token_to_label': self.cls_token_to_label
                },
                'timestamp': datetime.now().isoformat()
            }
            
            latest_path = self.checkpoint_dir / "latest_checkpoint.pt"
            torch.save(checkpoint, latest_path)
            
            if is_best:
                best_path = self.checkpoint_dir / "best_model_v4.pt"
                torch.save(checkpoint, best_path)
                self.logger.info(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: accuracy={self.best_accuracy:.4f}")
            
            self.logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜: epoch={epoch}, step={self.global_step}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
    
    def _load_checkpoint(self) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹ - æ–­ç‚¹æ¢å¤"""
        latest_path = self.checkpoint_dir / "latest_checkpoint.pt"
        
        if not latest_path.exists():
            self.logger.info("ğŸ“ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            return False
        
        try:
            self.logger.info(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {latest_path}")
            checkpoint = torch.load(latest_path, map_location=self.device)
            
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.global_step = checkpoint['global_step']
            self.best_accuracy = checkpoint['best_accuracy']
            
            if 'cls_mapping' in checkpoint:
                self.label_to_cls_token = checkpoint['cls_mapping']['label_to_token']
                self.cls_token_to_label = checkpoint['cls_mapping']['token_to_label']
            
            self.logger.info(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: epoch={self.current_epoch}, æœ€ä½³å‡†ç¡®ç‡={self.best_accuracy:.4f}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _start_monitoring(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        self.progress_thread = threading.Thread(target=self._progress_monitor, daemon=True)
        self.progress_thread.start()
        self.logger.info("âœ… ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def _progress_monitor(self):
        """è¿›åº¦ç›‘æ§çº¿ç¨‹"""
        while not self.stop_monitoring:
            try:
                if self.global_step > 0 and self.training_start_time:
                    elapsed = time.time() - self.training_start_time
                    steps_per_sec = self.global_step / elapsed if elapsed > 0 else 0
                    
                    total_steps = self.config.training.num_epochs * len(self.train_loader)
                    remaining_steps = total_steps - self.global_step
                    eta_seconds = remaining_steps / steps_per_sec if steps_per_sec > 0 else 0
                    eta = str(timedelta(seconds=int(eta_seconds)))
                    
                    progress_msg = (
                        f"ğŸ”„ V4è®­ç»ƒè¿›åº¦ - Epoch: {self.current_epoch}/{self.config.training.num_epochs} | "
                        f"Step: {self.global_step}/{total_steps} | Speed: {steps_per_sec:.2f} steps/s | "
                        f"ETA: {eta} | Best Acc: {self.best_accuracy:.4f}"
                    )
                    
                    self.logger.info(progress_msg)
                
                time.sleep(60)  # æ¯åˆ†é’Ÿè¾“å‡ºä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"âŒ è¿›åº¦ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(30)
    
    def _train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        classification_correct = 0
        total_valid_samples = 0
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {epoch+1}/{self.config.training.num_epochs}",
            leave=False,
            disable=not sys.stdout.isatty()
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # éªŒè¯è®­ç»ƒæ ·æœ¬
            self._validate_training_sample(batch, batch_idx)
            
            # æ•°æ®å‡†å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            cls_positions = batch['cls_position']
            cls_targets = batch['cls_target_token'].to(self.device)
            cls_labels = batch['cls_label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            with autocast():
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                # ä½¿ç”¨è®­ç»ƒç›®æ ‡ç±»è®¡ç®—æŸå¤±
                total_loss_batch, loss_components = self.training_objective.compute_loss(
                    outputs.logits, outputs.loss, cls_positions, cls_targets, cls_labels
                )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            self.scaler.scale(total_loss_batch).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()
            
            # ç»Ÿè®¡
            total_loss += loss_components['total_loss']
            classification_correct += loss_components['classification_accuracy'] * loss_components['valid_samples']
            total_valid_samples += loss_components['valid_samples']
            self.global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if sys.stdout.isatty():
                current_acc = classification_correct / max(total_valid_samples, 1)
                progress_bar.set_postfix({
                    'Loss': f'{loss_components["total_loss"]:.4f}',
                    'Acc': f'{current_acc:.4f}',
                    'LR': f'{self.scheduler.get_last_lr()[0]:.6f}'
                })
            
            # å®šæœŸä¿å­˜å’ŒéªŒè¯
            if self.global_step % 500 == 0:
                self._save_checkpoint(epoch)
            
            if self.global_step % 100 == 0:
                val_accuracy = self._validate()
                if val_accuracy > self.best_accuracy:
                    self.best_accuracy = val_accuracy
                    self._save_checkpoint(epoch, is_best=True)
                self.model.train()
        
        epoch_loss = total_loss / len(self.train_loader)
        epoch_accuracy = classification_correct / max(total_valid_samples, 1)
        
        self.logger.info(f"ğŸ“Š Epoch {epoch+1} ç»“æœ: æŸå¤±={epoch_loss:.4f}, å‡†ç¡®ç‡={epoch_accuracy:.4f}")
        return epoch_loss, epoch_accuracy
    
    def _validate(self) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                cls_positions = batch['cls_position']
                cls_targets = batch['cls_target_token'].to(self.device)
                
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits
                
                for i, cls_pos in enumerate(cls_positions):
                    if cls_pos != -1:
                        cls_logits = logits[i, cls_pos, :]
                        pred_token = torch.argmax(cls_logits)
                        if pred_token == cls_targets[i]:
                            total_correct += 1
                        total_samples += 1
        
        accuracy = total_correct / total_samples if total_samples > 0 else 0.0
        self.logger.info(f"ğŸ¯ éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f} ({total_correct}/{total_samples})")
        return accuracy
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸš€ å¼€å§‹V4è®­ç»ƒ...")
        self.logger.info("=" * 80)
        print(self.training_objective.get_objective_summary())
        self.logger.info("=" * 80)
        
        try:
            self.training_start_time = time.time()
            
            # åˆå§‹åŒ–
            self._initialize_model()
            self._load_and_prepare_data()
            
            # å°è¯•æ¢å¤
            resumed = self._load_checkpoint()
            if resumed:
                self.logger.info("ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
            
            # å¯åŠ¨ç›‘æ§
            self._start_monitoring()
            
            # è®­ç»ƒå¾ªç¯
            for epoch in range(self.current_epoch, self.config.training.num_epochs):
                self.current_epoch = epoch
                self.logger.info(f"\nğŸš€ å¼€å§‹ Epoch {epoch+1}/{self.config.training.num_epochs}")
                
                train_loss, train_acc = self._train_epoch(epoch)
                val_accuracy = self._validate()
                
                if val_accuracy > self.best_accuracy:
                    self.best_accuracy = val_accuracy
                    self._save_checkpoint(epoch, is_best=True)
                else:
                    self._save_checkpoint(epoch)
                
                self.logger.info(f"âœ… Epoch {epoch+1} å®Œæˆ | æœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.4f}")
            
            # è®­ç»ƒå®Œæˆ
            total_time = time.time() - self.training_start_time
            self.logger.info("\n" + "="*80)
            self.logger.info("ğŸ‰ V4è®­ç»ƒå®Œæˆ!")
            self.logger.info(f"ğŸ“Š æ€»æ—¶é—´: {str(timedelta(seconds=int(total_time)))}")
            self.logger.info(f"ğŸ“Š æœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.4f}")
            self.logger.info(f"ğŸ“Š æ€»æ­¥æ•°: {self.global_step}")
            self.logger.info("=" * 80)
            
        except KeyboardInterrupt:
            self.logger.info("\nâš ï¸ è®­ç»ƒè¢«ä¸­æ–­")
            self._save_checkpoint(self.current_epoch)
        except Exception as e:
            self.logger.error(f"\nâŒ è®­ç»ƒé”™è¯¯: {e}")
            self.logger.error(traceback.format_exc())
            self._save_checkpoint(self.current_epoch)
            raise
        finally:
            self.stop_monitoring = True

def setup_signal_handlers(trainer):
    """è®¾ç½®ä¿¡å·å¤„ç†"""
    def signal_handler(signum, frame):
        print(f"\nğŸ›‘ æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œä¼˜é›…åœæ­¢...")
        trainer.stop_monitoring = True
        trainer._save_checkpoint(trainer.current_epoch)
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Training V4 - ç»ˆæè®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)
    print("âœ… æŒ‰ç…§ç”¨æˆ·è¦æ±‚å®ç°:")
    print("   1. è·‘æ»¡GPU - æ··åˆç²¾åº¦+æ•°æ®å¹¶è¡Œ+ä¼˜åŒ–æ‰¹æ¬¡")
    print("   2. é™é»˜è®­ç»ƒ - åå°è¿è¡Œ+æ—¥å¿—é‡å®šå‘")
    print("   3. æ–­ç‚¹å¯æ¢å¤ - è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹")
    print("   4. ç»ˆç«¯å…³é—­å¯æŒç»­ - ä¿¡å·å¤„ç†")
    print("   5. éšæ—¶è¾“å‡ºè¿›åº¦ - å¤šçº¿ç¨‹ç›‘æ§")
    print("   6. è®­ç»ƒé€»è¾‘æ­£ç¡® - æ˜ç¡®åˆ†ç±»ç›®æ ‡")
    print("   7. 1024å…¨å±€é•¿åº¦ - ä¿æŠ¤CLS token")
    print("=" * 80)
    
    try:
        trainer = V4Trainer()
        setup_signal_handlers(trainer)
        
        print(f"\nğŸ¯ è®­ç»ƒé…ç½®ç¡®è®¤:")
        print(f"   - åºåˆ—é•¿åº¦: {trainer.config.model.n_positions}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {trainer.config.training.batch_size}")
        print(f"   - ç›®æ ‡å‡†ç¡®ç‡: >20%")
        
        trainer.train()
        return 0
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
