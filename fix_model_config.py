#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤æ¨¡å‹é…ç½®æ–‡ä»¶ï¼Œç¡®ä¿æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½
"""

import json
import os
import torch
from transformers import GPT2Config, GPT2Tokenizer
import pickle

def fix_model_configs():
    """ä¿®å¤æ¨¡å‹é…ç½®æ–‡ä»¶"""
    print("ğŸ”§ å¼€å§‹ä¿®å¤æ¨¡å‹é…ç½®æ–‡ä»¶...")
    
    # ä»æ£€æŸ¥ç‚¹åŠ è½½é…ç½®ä¿¡æ¯
    checkpoint_path = 'outputs/checkpoints/checkpoint.pkl'
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹åŠ è½½é…ç½®ä¿¡æ¯: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸï¼Œè½®æ¬¡: {checkpoint.get('epoch', 'unknown')}")
    else:
        print("âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # åŠ è½½è¯æ±‡è¡¨ä¿¡æ¯
    vocab_path = 'generated_sequences_super_enhanced/vocab.json'
    if os.path.exists(vocab_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        vocab_size = len(vocab) + 50257  # è‡ªå®šä¹‰vocab + GPT2é»˜è®¤vocab
        print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    else:
        vocab_size = 50773  # ä»è®­ç»ƒæ—¥å¿—ä¸­è·å–çš„å¤§å°
        print(f"âš ï¸ ä½¿ç”¨é»˜è®¤è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    
    # åˆ›å»ºæ­£ç¡®çš„é…ç½®
    config_dict = {
        "architectures": ["GPT2LMHeadModel"],
        "model_type": "gpt2",
        "vocab_size": vocab_size,
        "n_positions": 1024,
        "n_embd": 384,
        "n_layer": 6,
        "n_head": 8,
        "n_inner": 1536,  # é€šå¸¸æ˜¯ n_embd * 4
        "activation_function": "gelu_new",
        "resid_pdrop": 0.1,
        "embd_pdrop": 0.1,
        "attn_pdrop": 0.1,
        "layer_norm_epsilon": 1e-05,
        "initializer_range": 0.02,
        "summary_type": "cls_index",
        "summary_use_proj": True,
        "summary_activation": None,
        "summary_proj_to_labels": True,
        "summary_first_dropout": 0.1,
        "scale_attn_weights": True,
        "use_cache": True,
        "scale_attn_by_inverse_layer_idx": False,
        "reorder_and_upcast_attn": False,
        "bos_token_id": 50256,
        "eos_token_id": 50256,
        "pad_token_id": 50257,  # å‡è®¾PAD tokenæ˜¯ç¬¬ä¸€ä¸ªæ·»åŠ çš„ç‰¹æ®Štoken
        "transformers_version": "4.41.0"
    }
    
    # ä¿®å¤æœ€ä½³æ¨¡å‹ç›®å½•
    best_model_dir = 'outputs/best_model'
    if os.path.exists(best_model_dir):
        print(f"ğŸ”§ ä¿®å¤æœ€ä½³æ¨¡å‹é…ç½®: {best_model_dir}")
        
        # ä¿å­˜config.json
        config_path = os.path.join(best_model_dir, 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
        
        # å¤åˆ¶tokenizeræ–‡ä»¶åˆ°æœ€ä½³æ¨¡å‹ç›®å½•
        final_model_dir = 'outputs/final_model'
        if os.path.exists(final_model_dir):
            tokenizer_files = [
                'vocab.json',
                'merges.txt', 
                'added_tokens.json',
                'special_tokens_map.json',
                'tokenizer_config.json'
            ]
            
            for file_name in tokenizer_files:
                src_path = os.path.join(final_model_dir, file_name)
                dst_path = os.path.join(best_model_dir, file_name)
                
                if os.path.exists(src_path):
                    import shutil
                    shutil.copy2(src_path, dst_path)
                    print(f"âœ… å¤åˆ¶æ–‡ä»¶: {file_name}")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
    
    # ä¿®å¤æœ€ç»ˆæ¨¡å‹ç›®å½•
    final_model_dir = 'outputs/final_model'
    if os.path.exists(final_model_dir):
        print(f"ğŸ”§ ä¿®å¤æœ€ç»ˆæ¨¡å‹é…ç½®: {final_model_dir}")
        
        # ä¿å­˜config.json
        config_path = os.path.join(final_model_dir, 'config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from transformers import GPT2LMHeadModel, GPT2Tokenizer
        
        # æµ‹è¯•åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_dir = 'outputs/best_model'
        if os.path.exists(os.path.join(best_model_dir, 'config.json')):
            print(f"ğŸ“‚ æµ‹è¯•åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_dir}")
            
            # åŠ è½½é…ç½®
            config = GPT2Config.from_pretrained(best_model_dir)
            print(f"âœ… é…ç½®åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
            
            # åŠ è½½tokenizer
            tokenizer = GPT2Tokenizer.from_pretrained(best_model_dir)
            print(f"âœ… TokenizeråŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
            
            # åŠ è½½æ¨¡å‹
            model = GPT2LMHeadModel.from_pretrained(best_model_dir)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
            
            print("ğŸ‰ æœ€ä½³æ¨¡å‹ä¿®å¤æˆåŠŸï¼Œå¯ä»¥æ­£å¸¸åŠ è½½ï¼")
            return True
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ä¿®å¤GPTæ¨¡å‹é…ç½®æ–‡ä»¶...")
    
    # ä¿®å¤é…ç½®æ–‡ä»¶
    if fix_model_configs():
        print("\nâœ… é…ç½®æ–‡ä»¶ä¿®å¤å®Œæˆï¼")
        
        # æµ‹è¯•åŠ è½½
        if test_model_loading():
            print("\nğŸ‰ æ¨¡å‹ä¿®å¤æˆåŠŸï¼ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹ï¼š")
            print("```python")
            print("from transformers import GPT2LMHeadModel, GPT2Tokenizer")
            print("model = GPT2LMHeadModel.from_pretrained('outputs/best_model')")
            print("tokenizer = GPT2Tokenizer.from_pretrained('outputs/best_model')")
            print("```")
        else:
            print("\nâš ï¸ é…ç½®ä¿®å¤å®Œæˆï¼Œä½†æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    else:
        print("\nâŒ é…ç½®æ–‡ä»¶ä¿®å¤å¤±è´¥")
